[
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/5.4.1-architecture/",
	"title": "Architecture &amp; Tech Stack",
	"tags": [],
	"description": "",
	"content": "The system is built upon a Serverless architecture on AWS to optimize operational costs and scalability.\n1. Core Technology Stack Component Selected Technology Rationale Language Python 3.x Specialized AI Ecosystem. The optimal deployment environment for LLMs, Embeddings, and RAG workflows. Compute AWS Lambda Event-Driven Serverless Architecture. Optimizes operational costs (pay-per-request) and offers seamless integration with the AWS ecosystem (S3, API Gateway). Raw Storage Amazon S3 Durable \u0026amp; Cost-Effective Storage. Securely stores raw data, integrated directly with Lambda automated processing triggers. Vector DB Pinecone Managed Service. Specialized for Vector Search, offering high query speeds, low latency, and zero infrastructure management. Meta DB Amazon DynamoDB Millisecond Latency. Optimized for Exact Match queries and storing conversation history with instant response speeds. AI Model Amazon Bedrock Unified API. Access to diverse Foundation Models via a single gateway, ensuring absolute data privacy and security. CI/CD GitHub Actions Automation. Automates the Testing and Deployment process to Lambda immediately upon code push. 2. Resources \u0026amp; Environment Setup Before proceeding with the detailed implementation, ensure the following resources and access rights are established:\nAWS Account \u0026amp; Region:\nAn active AWS account with billing enabled. Third-Party Services:\nPinecone: Create an account and generate an API Key (Serverless Index). GitHub: Configure GitHub Secrets to securely store credentials (AWS_ACCESS_KEY, PINECONE_API_KEY) for CI/CD pipelines. Local Environment:\nInstall AWS CLI v2 and run aws configure. Install Python 3.9+ and SAM CLI (Serverless Application Model) for building and deploying Lambda functions. "
},
{
	"uri": "//localhost:1313/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: - This section is dedicated to translated blogs. - English versions of the blog posts are not available. - For the full and localized content, please refer to the Vietnamese translated versions of each blog.\n"
},
{
	"uri": "//localhost:1313/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: - This section is dedicated to translated blogs. - English versions of the blog posts are not available. - For the full and localized content, please refer to the Vietnamese translated versions of each blog.\n"
},
{
	"uri": "//localhost:1313/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: - This section is dedicated to translated blogs. - English versions of the blog posts are not available. - For the full and localized content, please refer to the Vietnamese translated versions of each blog.\n"
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders Time: Thursday, September 18, 2025, 9:00 – 17:30 Location: Amazon Web Services Vietnam, 36th Floor, 2 Hai Trieu, Ben Nghe Ward, District 1, Ho Chi Minh City Role: Attendee\nEvent Objectives Update on top strategic technology trends: Agentic AI. Learn about Data Foundation solutions to address the \u0026ldquo;Data Silos\u0026rdquo; problem that 52% of CDOs are facing. Approach the new software development process: AI-Driven Development Lifecycle (AI-DLC). Grasp security standards for GenAI (MITRE ATLAS, OWASP, NIST) and risk layers. Speakers Eric Yeo - Country General Manager, AWS Vietnam Dr. Jens Lottner - CEO, Techcombank Ms. Trang Phung - CEO \u0026amp; Co-Founder, U2U Network Jaime Valles - VP, GM Asia Pacific and Japan, AWS Jeff Johnson - Managing Director, ASEAN, AWS Vu Van - Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh - Chairman, Nexttech Group Dieter Botha - CEO, TymeX Jun Kai Loke - AI/ML Specialist SA, AWS Kien Nguyen - Solutions Architect, AWS Tamelly Lim - Storage Specialist SA, AWS Binh Tran - Senior Solutions Architect, AWS Taiki Dang - Solutions Architect, AWS Michael Armentano - Principal WW GTM Specialist, AWS Hung Nguyen Gia - Head of Solutions Architect, AWS Son Do - Technical Account Manager, AWS Nguyen Van Hai - Director of Software Engineering, Techcombank Phuc Nguyen - Solutions Architect, AWS Alex Tran - AI Director, OCB Nguyen Minh Ngan - AI Specialist, OCB Nguyen Manh Tuyen - Head of Data Application, LPBank Securities Vinh Nguyen - Co-Founder \u0026amp; CTO, Ninety Eight Hung Hoang - Customer Solutions Manager, AWS Christal Poon - Specialist Solutions Architect, AWS Key Highlights 1. Agentic AI \u0026amp; Data Strategy Trends Current State: 88% of CDOs are moving forward with GenAI, yet 52% state their data foundation is not ready. Challenges: Businesses are held back by 3 types of \u0026ldquo;Silos\u0026rdquo;: Data Silos, People Silos, and Business Silos. Data Strategy: End-to-end model from Producers → Foundations → Consumers. Infrastructure: Amazon S3 (Data Lakes), Amazon Redshift (Data Warehouses), supporting open standard Apache Iceberg. Governance: Amazon DataZone assists with Data \u0026amp; AI Governance. New Tools: Introduction of Unified Studio integrating analytics and AI tools. 2. AI-Driven Development Lifecycle (AI-DLC) Speaker Binh Tran introduced the shift from AI-Assisted to AI-Driven Development. The AI-DLC process consists of 3 main phases:\nInception: Build Context on existing code, clarify intent (User Stories), plan (Units of Work), and Domain Modeling. Construction: AI generates code \u0026amp; Tests, adds architectural components. Operation: Deploy with IaC \u0026amp; tests, incident management. 3. Security for GenAI Speaker Taiki Dang emphasized that security must run in parallel with Generative AI.\nRisk Layers: Top layer (Consumer): Risks regarding IP, legal, hallucinations, safety. Middle layer (Tuner): Risks from Managed services, data retention policies. Bottom layer (Provider): Risks from training data. Frameworks \u0026amp; Standards: Apply MITRE ATLAS, OWASP Top 10 for LLM, NIST AI RMF, ISO 42001. Solutions: Use Amazon Bedrock Guardrails to prevent and mitigate risks (such as toxicity, PII leaks). 4. Analytics \u0026amp; Business Intelligence Speaker Christal Poon presented the transition from Amazon QuickSight to Amazon Q.\nFeatures for creating Dashboards, reports, and Data QA using natural language. Coming soon to Vietnam: Amazon Agentic AI Workbench (Quick Suite) with Quick Researcher and Quick Automate capabilities, keeping humans in the loop for control. Key Takeaways Agent Mindset: Clearly understand the structure of an AI Agent: Goals → Observation → Tools → Context → Action. Agent Core Architecture: Ensure all components are present: Runtime, Gateway, Memory, Observability, and Identity to deploy Agents to production safely and scalably. Multi-layer Security: Security extends beyond the application; it must control risks from training data and fine-tuning processes down to end-users (Consumer risks). Applying to Work Implement AI-DLC: Pilot the 7-step AI-DLC process in a new project, starting with using AI to \u0026ldquo;Build Context\u0026rdquo; and \u0026ldquo;Domain Modeling\u0026rdquo;. Enhance Security: Review current GenAI applications against the OWASP Top 10 for LLM checklist and integrate Bedrock Guardrails to filter harmful content. Modernize Data Stack: Evaluate the feasibility of migrating the current Data Warehouse to a Lakehouse architecture with Apache Iceberg on AWS to break down Data Silos. Personal Experience This event went much deeper technically (deep-dive) than I expected, providing significant practical value:\nI was very impressed by the sharing on AI-DLC by Mr. Binh Tran. It completely changed my perspective on coding: developers are no longer just writing code but becoming \u0026ldquo;architects\u0026rdquo; and \u0026ldquo;reviewers\u0026rdquo; for AI execution. The Scoping Matrix slide and the risk layers in GenAI gave me a more systematic view to justify new AI project deployments to the company\u0026rsquo;s Security department. I am very excited about the news that Amazon Agentic AI Workbench is coming to Vietnam, promising to solve the problem of automating market research processes (Quick Researcher) that the business team currently needs. Event Photos Summary: A \u0026ldquo;must-attend\u0026rdquo; event for Builders. Knowledge about Agentic AI and AI-DLC will be the compass for my technical development roadmap in the coming year.\n"
},
{
	"uri": "//localhost:1313/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Nguyen Van Linh\nPhone Number: 037 393 9617\nEmail: LinhNVSE184598@fpt.edu.vn\nUniversity: FPT University HCMC\nMajor: Artificial Intelligence\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 08/12/2025\nx\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "//localhost:1313/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "SorcererXtreme AI: Building an AI-Powered Metaphysical Guidance Platform on AWS The core purpose of this project, for a developer workshop, is to demonstrate how to build a scalable, cost-optimized, multi-faceted application capable of handling complex data flows entirely within the cloud environment.\nProblem Solved \u0026amp; Technical Value The Challenge: Building a platform that combines the need for precise computation with the linguistic creativity of AI, while ensuring all content is verifiable and grounded. Traditional server-based solutions often struggle with the dynamic scaling required for such varied workloads. The Technical Solution: We solve this by implementing a Retrieval-Augmented Generation (RAG) Core utilizing Amazon Bedrock and Pinecone. This design allows the AI to produce verified answers based on a specialized knowledge base, transforming speculative guidance into actionable insights. Key Technical Highlights This project serves as an essential case study for integrating the following critical AWS services:\nServerless Compute: We utilize AWS Lambda as the entire Backend, eliminating server management overhead and significantly optimizing costs. Modern Deployment (Frontend Hosting): Deploying the Next.js application on AWS Amplify provides streamlined CI/CD and hosting for the Frontend. Durable Asynchronous Flow (Async): We constructed a reliable automated reminder system using EventBridge -\u0026gt; Lambda -\u0026gt; SES. This pattern ensures robust, scalable bulk delivery without overloading the core API. Data Persistence and Vectors: We manage complex relational data externally using NeonDB (Serverless PostgreSQL) while utilizing a specialized Vector Database (Pinecone) for the high-speed RAG retrieval layer. Security and DevOps: AWS Parameter Store manages all sensitive keys, and the entire infrastructure is deployed using the Serverless Framework driven by GitHub Actions (CI/CD). Best Practices Learned Attendees will learn how to implement a fully Serverless Microservices architecture, address challenges like external database connectivity, splitting synchronous/asynchronous workloads, and building a cost-effective RAG Core solution.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.1-prepare/",
	"title": "Preparation",
	"tags": [],
	"description": "",
	"content": "1. Technologies Used Category Technology Detail \u0026amp; Role Language TypeScript (Node.js 20) Primary language, providing Type Safety. Backend Core Express.js + serverless-http Familiar API Framework, \u0026ldquo;wrapped\u0026rdquo; to run on Lambda. Infrastructure (IaC) Serverless Framework V4 Main tool for defining and deploying the entire AWS architecture. Compute AWS Lambda Handles business logic and runs the TypeScript code. API Gateway AWS API Gateway Synchronous HTTP communication gateway for the entire Backend. Database NeonDB (Serverless PostgreSQL) Primary database for relational data. ORM Prisma Abstraction Layer between the code and the database. Security AWS SSM Parameter Store Secure storage for sensitive environment variables. DevOps GitHub Actions Automates the CI/CD pipeline. 2. Required Resources \u0026amp; Software To complete the workshop, users must have the following tools and accounts ready on their computer.\nA. Account Requirements AWS Account: Necessary for deploying Serverless services (Lambda, API Gateway, SSM). NeonDB Account: Necessary for creating and retrieving the connection string (DATABASE_URL) for the PostgreSQL database. GitHub Account: Necessary for storing the source code and setting up CI/CD (GitHub Actions). B. Local Software \u0026amp; Tools Node.js (v20+): Installed and accessible via the terminal. npm or yarn: Package manager. AWS CLI: Necessary to configure AWS access permissions from the local machine for the Serverless Framework. IDE (VS Code): Recommended development environment. Postman/Insomnia: Essential tool for testing API Endpoints (GET/POST). C. Project Setup Serverless Framework CLI: Must be installed globally (npm install -g serverless). AWS Credentials: Configure AWS access permissions (User/Role) on the local machine. "
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.1-preparation/",
	"title": "Setting up Development Environment",
	"tags": [],
	"description": "",
	"content": "1. Prerequisites To develop modern web applications, you need the following standard tools:\nNode.js (LTS Version): Runtime environment for JavaScript/TypeScript. Git: Distributed version control system. IDE: Visual Studio Code (recommended). Recommended VS Code Extensions:\nESLint \u0026amp; Prettier: Automate formatting and linting. Tailwind CSS IntelliSense: Rapid Tailwind class suggestions. ES7+ React/Redux/React-Native snippets: Code faster with shortcuts. 2. Initialize Next.js Project We will use Next.js - the most popular React Framework today.\nRun initialization command:\nnpx create-next-app@latest my-serverless-app Detailed Configuration:\nTypeScript: Yes (Type safety) Tailwind CSS: Yes (Rapid styling) ESLint: Yes (Linting) App Router: Yes (Latest routing architecture) Import Alias: @/ (Cleaner imports) 3. Initialize AWS Amplify (Backend) This is a crucial step to integrate Serverless features (Auth, Data) into your project. Run the following command inside your project folder:\ncd my-serverless-app npm create amplify@latest When prompt to install, select Yes. Amplify will automatically create the amplify/ folder containing the Backend structure.\n4. Install AWS Libraries Install necessary SDK packages for Frontend to communicate with AWS:\nnpm install aws-amplify @aws-amplify/ui-react 5. Standard Project Structure After installation, your folder structure should look like this:\namplify/: Contains Backend code (auth.ts, data.ts). src/app: Contains Pages and Layouts (App Router). src/components: Contains reusable UI Components. amplify_outputs.json: Auto-generated config file (Do not edit). 6. Configure tsconfig.json (Best Practices) To ensure strict TypeScript coding, update tsconfig.json:\n{ \u0026#34;compilerOptions\u0026#34;: { \u0026#34;target\u0026#34;: \u0026#34;es5\u0026#34;, \u0026#34;lib\u0026#34;: [\u0026#34;dom\u0026#34;, \u0026#34;dom.iterable\u0026#34;, \u0026#34;esnext\u0026#34;], \u0026#34;allowJs\u0026#34;: true, \u0026#34;skipLibCheck\u0026#34;: true, \u0026#34;strict\u0026#34;: true, \u0026#34;forceConsistentCasingInFileNames\u0026#34;: true, \u0026#34;noEmit\u0026#34;: true, \u0026#34;esModuleInterop\u0026#34;: true, \u0026#34;module\u0026#34;: \u0026#34;esnext\u0026#34;, \u0026#34;moduleResolution\u0026#34;: \u0026#34;node\u0026#34;, \u0026#34;resolveJsonModule\u0026#34;: true, \u0026#34;isolatedModules\u0026#34;: true, \u0026#34;jsx\u0026#34;: \u0026#34;preserve\u0026#34;, \u0026#34;incremental\u0026#34;: true, \u0026#34;plugins\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;next\u0026#34; } ], \u0026#34;paths\u0026#34;: { \u0026#34;@/*\u0026#34;: [\u0026#34;./src/*\u0026#34;] } }, \u0026#34;include\u0026#34;: [\u0026#34;next-env.d.ts\u0026#34;, \u0026#34;**/*.ts\u0026#34;, \u0026#34;**/*.tsx\u0026#34;, \u0026#34;.next/types/**/*.ts\u0026#34;], \u0026#34;exclude\u0026#34;: [\u0026#34;node_modules\u0026#34;] } 7. Install UI Libraries To create the \u0026ldquo;Mystical\u0026rdquo; UI:\nnpm install framer-motion lucide-react clsx tailwind-merge 8. Configure TailwindCSS Set up colors in tailwind.config.ts:\n// tailwind.config.ts import type { Config } from \u0026#34;tailwindcss\u0026#34;; const config: Config = { content: [ \u0026#34;./src/pages/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, \u0026#34;./src/components/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, \u0026#34;./src/app/**/*.{js,ts,jsx,tsx,mdx}\u0026#34;, ], theme: { extend: { colors: { primary: \u0026#34;#432c7a\u0026#34;, secondary: \u0026#34;#764ba2\u0026#34;, accent: \u0026#34;#ffd700\u0026#34;, background: \u0026#34;#1a0b2e\u0026#34;, }, backgroundImage: { \u0026#34;gradient-radial\u0026#34;: \u0026#34;radial-gradient(var(--tw-gradient-stops))\u0026#34;, }, }, }, plugins: [], }; export default config; My Experience Amplify Gen 2 vs Gen 1: If you used Amplify CLI (Gen 1) before with commands like amplify add auth, forget it! Gen 2 (what we are using) is Code-First. You define Backend using TypeScript (in amplify/ folder) instead of clicking through Console. It gives Frontend Devs much better control over infrastructure.\nVerification \u0026amp; Testing Test Case: Check Amplify Installation\nOpen package.json. Look inside dependencies. Expected Result: You should see \u0026quot;aws-amplify\u0026quot;: \u0026quot;^6.x.x\u0026quot; and \u0026quot;@aws-amplify/backend\u0026quot;: \u0026quot;^1.x.x\u0026quot;. "
},
{
	"uri": "//localhost:1313/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Attended the First Cloud Journey; introduced myself, set up communication channels with cohort members, and aligned on program timelines and expectations.\nJoined all First Cloud Journey groups; proactively networked and identified potential teammates for upcoming projects.\nStudied fundamental AWS services.\nTasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - proactively networked and identified potential teammates for upcoming projects 08/09/2025 08/09/2025 https://policies.fcjuni.com/ 3 - Researched AWS fundamentals and key service categories: + Compute + Storage + Networking + Database 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Designed AWS architecture using draw.io 10/09/2025 10/09/2025 https://youtu.be/l8isyDe-GwY?si=vOVD-KKoCTmOG6-a 5 - Explore cloud computing: + Cost optimization + Accelerating development - Learn about AWS global infrastructure 11/09/2025 11/09/2025 https://youtu.be/IK59Zdd1poE?si=15Tp-TUn3Cix3F0d https://youtu.be/pjr5a-HYAjI?si=b289Mn48LI5Uyheu https://youtu.be/IY61YlmXQe8?si=H_sZ7pl4i-97vuMO https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Created an AWS Free Tier account 12/09/2025 13/09/2025 https://000001.awsstudygroup.com/ Week 1 Achievements: Gained understanding of AWS and its core service categories:\nProvides processing power (VMs, containers, serverless) to run applications. Scalable data storage (object, block, file) for various workloads. Networking services (VPC, load balancing, DNS) for connectivity, security, and performance. Managed database engines (SQL, NoSQL, in-memory) for application data management. Gained the ability to design AWS architecture diagrams in draw.io with proper layout and standard notation.\nGained a solid understanding of cloud computing\nMethods for cost optimization (right-sizing, reserved instance, saving plan, spot, serverless, cost allocation tag, AWS Budget, etc.). Recognized how cloud computing enables faster development and agile deployment. Explored the different AWS Support Plans (Basic, Developer, Business, Enterprise) and their use cases. Acquired knowledge of the AWS Global Infrastructure, including Data Centers, Availability Zones (AZ), Regions, and Edge Locations. Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\n"
},
{
	"uri": "//localhost:1313/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.2-ui-implementation/",
	"title": "Building Modern User Interface",
	"tags": [],
	"description": "",
	"content": "1. Layout Design (Responsive \u0026amp; Glassmorphism) Our goal is to create a mystical, \u0026ldquo;AI Sorcerer\u0026rdquo; interface like the Mockup below:\nUse CSS Grid and Flexbox from Tailwind to create flexible layouts.\n// src/app/layout.tsx export default function RootLayout({ children }: { children: React.ReactNode }) { return ( \u0026lt;html lang=\u0026#34;en\u0026#34;\u0026gt; \u0026lt;body className=\u0026#34;bg-background text-white min-h-screen bg-[url(\u0026#39;/bg-stars.png\u0026#39;)] bg-cover\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;absolute inset-0 bg-black/50\u0026#34; /\u0026gt; {/* Overlay */} \u0026lt;main className=\u0026#34;relative z-10 container mx-auto px-4 py-8 grid grid-cols-1 lg:grid-cols-12 gap-6\u0026#34;\u0026gt; {/* Sidebar takes 3 cols on Desktop */} \u0026lt;aside className=\u0026#34;lg:col-span-3 hidden lg:block\u0026#34;\u0026gt; {/* Sidebar Content */} \u0026lt;/aside\u0026gt; {/* Main Content takes 9 cols */} \u0026lt;section className=\u0026#34;lg:col-span-9\u0026#34;\u0026gt; {children} \u0026lt;/section\u0026gt; \u0026lt;/main\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; ) } 2. Component: Horoscope Widget Create a Widget displaying daily horoscope with Glassmorphism effect.\n// src/components/HoroscopeWidget.tsx import { Star } from \u0026#39;lucide-react\u0026#39;; export default function HoroscopeWidget({ sign, prediction }: { sign: string, prediction: string }) { return ( \u0026lt;div className=\u0026#34;p-6 rounded-2xl bg-white/10 backdrop-blur-lg border border-white/20 hover:bg-white/20 transition-all duration-300 group\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex items-center gap-3 mb-4\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;p-3 rounded-full bg-accent/20 text-accent group-hover:scale-110 transition-transform\u0026#34;\u0026gt; \u0026lt;Star size={24} /\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;h3 className=\u0026#34;text-xl font-bold\u0026#34;\u0026gt;{sign}\u0026lt;/h3\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;p className=\u0026#34;text-gray-300 leading-relaxed\u0026#34;\u0026gt;{prediction}\u0026lt;/p\u0026gt; \u0026lt;/div\u0026gt; ); } 3. Component: Chat Interface Build chat interface with auto-scroll and dynamic message styling.\n// src/components/ChatBox.tsx import { useEffect, useRef } from \u0026#39;react\u0026#39;; import { clsx } from \u0026#39;clsx\u0026#39;; export default function ChatBox({ messages }: { messages: Message[] }) { const bottomRef = useRef\u0026lt;HTMLDivElement\u0026gt;(null); // Auto-scroll to bottom useEffect(() =\u0026gt; { bottomRef.current?.scrollIntoView({ behavior: \u0026#39;smooth\u0026#39; }); }, [messages]); return ( \u0026lt;div className=\u0026#34;flex flex-col h-[600px] bg-white/5 rounded-xl border border-white/10 overflow-hidden\u0026#34;\u0026gt; \u0026lt;div className=\u0026#34;flex-1 overflow-y-auto p-4 space-y-4\u0026#34;\u0026gt; {messages.map((msg, idx) =\u0026gt; ( \u0026lt;div key={idx} className={clsx( \u0026#34;max-w-[80%] p-3 rounded-lg\u0026#34;, msg.role === \u0026#39;user\u0026#39; ? \u0026#34;bg-primary self-end ml-auto\u0026#34; : \u0026#34;bg-white/10 self-start mr-auto\u0026#34; )}\u0026gt; {msg.content} \u0026lt;/div\u0026gt; ))} \u0026lt;div ref={bottomRef} /\u0026gt; \u0026lt;/div\u0026gt; {/* Input Area */} \u0026lt;/div\u0026gt; ); } 4. Animation with Framer Motion Add smooth entrance effects for UI elements.\nimport { motion } from \u0026#39;framer-motion\u0026#39;; const fadeIn = { hidden: { opacity: 0, y: 20 }, visible: { opacity: 1, y: 0 } }; \u0026lt;motion.div initial=\u0026#34;hidden\u0026#34; animate=\u0026#34;visible\u0026#34; variants={fadeIn} transition={{ duration: 0.5 }} \u0026gt; \u0026lt;HoroscopeWidget sign=\u0026#34;Leo\u0026#34; prediction=\u0026#34;Today is your lucky day!\u0026#34; /\u0026gt; \u0026lt;/motion.div\u0026gt; My Experience Mobile-First or Desktop-First? Initially, I designed for Desktop first, and when I opened it on mobile, the layout was broken. Lesson: Always use classes like hidden lg:block or grid-cols-1 lg:grid-cols-12 to prioritize Mobile layout first (default), then override for larger screens. TailwindCSS makes this incredibly easy!\nVerification \u0026amp; Testing Test Case 1: Responsive Design\nOpen browser on Desktop: See Sidebar on left, Chatbox on right. Press F12, switch to Mobile mode (iPhone 12/14). Expected Result: Sidebar hidden, Chatbox takes full width. Test Case 2: Hover Effects\nHover over HoroscopeWidget. Expected Result: Background brightens (bg-white/20), star icon scales up slightly (scale-110). "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/5.4.2-data-strategy/",
	"title": "Data Strategy",
	"tags": [],
	"description": "",
	"content": "This section required the most intensive research effort to ensure the quality of AI responses (Garbage In, Garbage Out).\n1. Format Standardization Journey: From JSON to JSONL The choice of file storage format plays a decisive role in processing performance and AWS Lambda memory costs. After extensive testing, the system standardized the entire Knowledge Base (Tarot, Horoscope, Numerology) to the JSONL (.jsonl) format. In this format, each line of the file is a distinct, valid JSON object, independent of the others. Why JSONL? Memory Optimization: Utilizes Lazy Loading mechanisms instead of loading the entire file. This keeps Lambda RAM consumption low and stable, completely eliminating Out of Memory (OOM) errors with large datasets. Local Fault Tolerance: A syntax error in one line does not break the entire pipeline. The system automatically skips the erroneous line and continues processing, ensuring data flow continuity. Streaming Compatibility: Supports direct stream reading from S3, minimizing latency when initiating Batch processing. Comparison with Legacy Solution (Standard JSON) The initial standard JSON format revealed critical performance weaknesses:\nFeature Legacy Solution: Standard JSON (.json) Current Solution: JSONL (.jsonl) Structure - Monolithic Array. - Enclosed by [...], separated by commas. - Independent Lines. - Each line is a separate object. Performance - Memory Hog: Must load the entire file into RAM to parse the DOM. - Memory Safe: Processing consumes RAM per line only. Risk - Single Point of Failure: One missing comma = Entire file corrupted. - Isolated: Error in line 1 does not affect line 2. Structure Illustration:\nJSON (Legacy - Array): [ {\u0026#34;id\u0026#34;: 1, \u0026#34;text\u0026#34;: \u0026#34;Aries...\u0026#34;}, {\u0026#34;id\u0026#34;: 2, \u0026#34;text\u0026#34;: \u0026#34;Taurus...\u0026#34;} ] Figure 2.1: Data sample in standard JSON format.\nJSONL (New - Line-delimited): {\u0026#34;id\u0026#34;: 1, \u0026#34;text\u0026#34;: \u0026#34;Aries...\u0026#34;} {\u0026#34;id\u0026#34;: 2, \u0026#34;text\u0026#34;: \u0026#34;Taurus...\u0026#34;} Figure 2.2: Data configuration in JSONL format.\n2. \u0026ldquo;Divide and Conquer\u0026rdquo; Technique in Embedding Challenge: Raw text data is often too long and contains significant keyword noise. When RAG (Retrieval-Augmented Generation) queries entire large paragraphs, the AI\u0026rsquo;s focus is easily \u0026ldquo;diluted,\u0026rdquo; leading to rambling responses. Solution: Instead of embedding the entire text, the system implements: Flatten Contexts: Breaks down information fields within contexts (e.g., strengths, weaknesses, love). Meta-Injection: Attaches metadata (Name, Category, Keyword) to each micro-chunk before embedding. Result: During vector search, the system extracts the precise segment required (e.g., retrieving only the \u0026ldquo;Love aspects of Aries\u0026rdquo; segment rather than the entire Aries article), enabling the LLM to provide focused, accurate answers. 3. Database Design (DynamoDB) To ensure sub-10ms latency for Real-time applications, the system utilizes Amazon DynamoDB with a dual-table design, serving two core purposes: Knowledge Storage and Conversation Context Storage.\nFigure 3.1: Overview of active DynamoDB tables.\nMetaphysicalKnowledgeBase Table This acts as the \u0026ldquo;Source of Truth\u0026rdquo; for the system, storing detailed information on Tarot, Zodiac signs, and Numerology. Data here serves as the Ground Truth for the RAG process.\nPartition Key (PK): category (String). E.g., Tarot_card, Zodiac_sign. Enables efficient querying by grouping large datasets by type. Sort Key (SK): entity_name (String). E.g., Ace of Cups, Aries. Unique identifier for each entity within a category. Item Structure Detail: Instead of storing flat text, the system stores structured JSON within the contexts attribute. This allows flexible extraction of specific aspects (Love, Career, Health) rather than retrieving the entire document.\nFigure 3.2: Attribute details for a Tarot card (Ace of Cups).\nAttribute Name Type Description contexts String (JSON) Contains categorized content segments (general_upright, love_reversed, etc.). This is the primary source for Embedding. keywords List List of related keywords to support hybrid search alongside Semantic Search. Chat History Table (sorcererxtreme-chatMessages) To enable the AI to \u0026ldquo;remember\u0026rdquo; previous interactions (Context Retention), a short-term memory system is required. This table stores the entire conversation history per session.\nPartition Key (PK): sessionId (String). Unique identifier for the user\u0026rsquo;s chat session. Sort Key (SK): timestamp (String). Message timestamp (ISO 8601), ensuring conversation logs are ordered chronologically. Figure 3.3: Data sample of a stored user message.\nProcessing Flow: Whenever a user sends a new message:\nThe system queries this table using the current sessionId. Retrieves the last $N$ messages (Context Window). Injects this history into the LLM Prompt to ensure coherent, context-aware responses. "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop: Data Science on AWS Time: 9:30 – 11:45, October 16, 2025 Location: Hall A - FPT University HCMC (FPTU HCMC) Role: Attendee\nObjectives Understand the complete Landscape of AWS AI/ML services. Learn how to build, train, and deploy real-world Machine Learning models. Network with experts from the AWS User Group community. Speakers Van Hoang Kha - Cloud Solutions Architect, AWS User Group Leader Bach Doan Vuong - Cloud DevOps Engineer, AWS Community Builder Key Highlights 1. The AWS AI/ML Stack The speakers clearly introduced the 3-layer structure of the AWS AI/ML Stack:\nAI Services (Top layer): Ready-to-use services requiring no deep ML expertise (Vision, Speech, Chatbots, etc.). ML Services (Middle layer): Amazon SageMaker platform for building, training, and deploying models. Frameworks \u0026amp; Infrastructure (Bottom layer): For experts needing deep control (PyTorch, TensorFlow, EC2 GPU\u0026hellip;). 2. Featured AI Services \u0026amp; Demos I found this section incredibly interesting due to its high applicability for student projects:\nVision: Amazon Rekognition for face recognition, video analysis, PPE Detection, and Content Moderation. Speech \u0026amp; Audio: The combination of Amazon Polly (Text-to-Speech with lifelike voices) and Amazon Transcribe (Speech-to-Text, supporting real-time calls). Natural Language Processing (NLP): Amazon Translate: Fast, multi-language translation. Amazon Textract: Quickly extract data from scanned documents/invoices. Amazon Lex: Build intelligent conversational Chatbots. Personalization: Amazon Personalize creates real-time product recommendations without needing deep ML expertise. 3. Machine Learning Workflow with SageMaker The speakers demonstrated the process from Feature Engineering (raw data processing) to Training and Tuning parameters. Specifically, they showed how to bring existing Python code (Scikit-learn, TensorFlow) to run on AWS\u0026rsquo;s powerful infrastructure.\nPersonal Experience This workshop was very practical and relevant for students like me.\nI was particularly impressed by the sharing on Amazon Personalize. I used to think building a Recommendation System was extremely difficult and required complex algorithms, but AWS makes it much simpler. A huge plus was the speaker\u0026rsquo;s practical advice on Cost Management. The \u0026ldquo;Monitoring Cost Daily\u0026rdquo; slide made me realize that working with Cloud isn\u0026rsquo;t just about making code run, but also about optimizing costs to avoid \u0026ldquo;wallet shock\u0026rdquo; at the end of the month. The atmosphere at FPTU was energetic. Hearing experiences from industry seniors gave me more motivation to study for my upcoming AWS certifications. Event Photos Summary: This event truly \u0026ldquo;enlightened\u0026rdquo; me about the AI Services toolkit. Instead of building models from scratch, I now know how to leverage AWS\u0026rsquo;s available APIs to solve problems faster and more effectively.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/",
	"title": "Frontend Development",
	"tags": [],
	"description": "",
	"content": "Building Serverless Frontend with Next.js \u0026amp; AWS Amplify 1. Workshop Overview Welcome to the Building Modern Serverless Frontend workshop for the SorcererXtreme project.\nIn this project, the Frontend acts as the \u0026ldquo;face\u0026rdquo; of the application, where users directly interact with the mystical features. Our mission is to build a beautiful, smooth interface that communicates effectively with the backend AWS services.\n2. Frontend Architecture We will focus on the Client (Frontend) architecture and its Integration Points:\nFrontend Workflow: Hosting \u0026amp; Delivery: Next.js code is hosted and operated on AWS Amplify. Users access the web app via a global CDN (CloudFront) built into Amplify, ensuring lightning-fast load times. Authentication: When a user Logs In, the Frontend communicates directly with Amazon Cognito. Cognito returns a \u0026ldquo;Token\u0026rdquo; (like an access pass). API Interaction: For every request (like chatbot or Tarot reading), the Frontend sends this Token along with the request to Amazon API Gateway. Response: The Frontend receives JSON results from the API and renders the UI. The Frontend does not need to know what Database or AI model is behind the API; it only cares about the Input (Request) and Output (Response). 3. Frontend Tech Stack The \u0026ldquo;weaponry\u0026rdquo; of a Frontend Developer in this project:\nTechnology Role Why use it? Next.js (App Router) Framework Strong Server-Side Rendering (SSR) for SEO, powerful Router. AWS Amplify (Gen 2) Platform Provides Hosting, automated CI/CD, and fast Cloud connection libraries. Tailwind CSS Styling Rapid styling, easy customization for the mystical \u0026ldquo;Dark Mode\u0026rdquo;. Framer Motion Animation Create smooth motion effects (like 3D Tarot card flipping). Amplify UI Library Pre-built components for Login/Registration flows. Axios / Fetch HTTP Client Used to call API Gateway. 4. Estimated Time \u0026amp; Cost Item Details Time 2-3 hours per day Cost ~$9.06/month (Total project) 5. Workshop Content We will follow a standard Frontend development process:\nPreparation: Setup Next.js and Amplify. UI Implementation: Code Chat \u0026amp; Tarot UI with animations. Integration: Integrate Login (Cognito) and API calls (Gateway). CI/CD Pipeline: Push code to Git and auto-deploy to the Internet. Advanced: Custom Domain setup and SEO optimization. Backend Reference: Understanding RAG model. Cleanup: Cleaning up resources. Frontend Mindset: In Serverless architecture, Frontend is not just a \u0026ldquo;renderer\u0026rdquo;. It is also responsible for Security (keeping Tokens safe) and User Experience (handling Loading states while waiting for AI). Pay attention to these points during the workshop!\n"
},
{
	"uri": "//localhost:1313/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "\rAWS FIRST CLOUD AI JOURNEY – PROJECT PLAN TEEJ_SorcererXStreme - FPT University - Ho Chi Minh Campus - SORCERERXSTREME 0. TABLE OF CONTENTS CONTEXT AND MOTIVATION\n1.1. Summary 1.2. Project Success Criteria 1.3. Assumptions and Premises SOLUTION ARCHITECTURE\n2.1. Technical Architecture Diagram 2.2. Technical Plan 2.3. Project Plan 2.4. Security Factors ACTIVITIES AND DELIVERABLES\n3.1. Activities and Deliverables 3.2. Out of Scope 3.3. Operational Handover Process COST ANALYSIS BY SERVICE\nTEAM\nRESOURCES AND COST ESTIMATION\nACCEPTANCE\n1. CONTEXT AND MOTIVATION 1.1 Summary 1. Customer Context \u0026amp; Problem Problem: Current sources of spiritual information are fragmented, unverified, and lack personalization. Users struggle to find deep, reliable interpretations or compare Eastern vs. Western schools of knowledge (e.g., Eastern Horoscope vs. Astrology). Motivation: The urgent need of current customers is to build a unified platform capable of providing intellectually verified content while maintaining scalability and optimizing operational costs. 2. Business \u0026amp; Technical Goals Goal Type Details\rBusiness\r- Provide content with superior reliability and depth compared to current services. - Generate revenue from a paid service model.\rTechnical\r- Ensure AI Reliability: Apply RAG core to minimize AI \"hallucinations\". - Scalability: Build AWS Serverless architecture (Lambda, API Gateway) to easily handle high traffic and optimize operational costs (Pay-per-use).\r3. Use Cases Key functions the project will support for users:\nDeep Interactive AI Chat: Chat directly with AI, capable of maintaining context and combining multiple schools of thought in a single session. Personalized Interpretation: Provide in-depth reports based on input data (date of birth, place of birth, time of birth). Automated Notifications: Send periodic notifications via email. 4. Consulting Service Summary Professional services will be provided to achieve the above goals, including:\nServerless Architecture Design: Build a multi-model architecture and set up RAG flow on AWS Bedrock. Cost \u0026amp; Performance Optimization: Fine-tune Lambda functions and establish security via SSM Parameter Store. CI/CD Automation: Implement the entire development and deployment process automatically (IaC) using Serverless Framework and GitHub Actions. 1.2 Project Success Criteria The success of the project will be evaluated based on the following quantitative and qualitative criteria:\nData Reliability: The RAG system operates accurately, minimizing AI hallucinations, and providing transparent source citations for interpretations. Knowledge Unification: Successful integration of Eastern and Western metaphysical data into a single platform. Business Efficiency: Successful implementation of a tiered model (Free/VIP) to generate a stable revenue stream. Cost Optimization: System operates on Serverless architecture with an estimated cost of ~$9.06/month for the Demo environment. Scalability: System auto-scales to handle high traffic without manual intervention. 1.3 Assumptions and Premises The project is executed based on the following assumptions and constraints:\nTechnology Dependency: The project depends on the availability and stability of AWS services (Bedrock, Lambda, API Gateway). Data: It is assumed that input data for RAG (books, metaphysical documents) is clean, copyrighted, or within valid usage scope. Risks: There is a risk of LLM \u0026ldquo;hallucinations\u0026rdquo; despite using RAG; a Fact Checker mechanism is required. Cost Constraints: The operating budget is strictly optimized. 2. SOLUTION ARCHITECTURE 2.1 Technical Architecture Diagram The proposed architecture is Hybrid Serverless on AWS, including layers: Edge \u0026amp; Auth, API \u0026amp; Routing, Compute, Data, AI/ML, and Async Monitoring.\nKey Components:\nFrontend: AWS Amplify (Next.js). Auth: Amazon Cognito. Backend: AWS Lambda, Amazon API Gateway. Database: NeonDB (PostgreSQL) as main DB, DynamoDB for history, Pinecone for Vector Search. AI Core: Amazon Bedrock (LLM \u0026amp; Embeddings), S3 (RAG Docs). 2.2 Technical Plan The project team will develop and deploy the system according to the following technical process:\nScripts \u0026amp; IaC: Use Serverless Framework to generate CloudFormation templates, ensuring infrastructure deployment (IaC) is repeatable and consistent. CI/CD: Use GitHub Actions to automate the Build, Test, and Deploy process for Lambda functions and API Gateway. RAG Pipeline: Set up data processing flow: Upload documents to S3 -\u0026gt; Lambda Trigger -\u0026gt; Generate Embedding (Bedrock) -\u0026gt; Store in Pinecone Vector DB. 2.3 Project Plan The project applies the Agile-Iterative model over 9 weeks, divided into 3 main phases (Iterations):\nIter 3 (Weeks 1-3): Redesign system, finalize SRS/SDS documentation, and build Prototype for RAG pipeline. Iter 4 (Weeks 4-6): Integrate AWS Cognito, develop permission logic (Guest/VIP), build data corpus on S3. Iter 5 (Weeks 7-9): Full deployment to AWS, End-to-End testing (QA), cost and performance optimization. 2.4 Security Factors Security is designed according to the \u0026ldquo;Defense in Depth\u0026rdquo; model:\nIdentity \u0026amp; Access: Use Amazon Cognito to manage identity and user permissions (User Roles). Data Protection: Secret keys (API Keys, DB Credentials) are stored securely in AWS Systems Manager Parameter Store, not hardcoded. Network Security: API Gateway acts as the sole entry point. Monitoring: Use CloudWatch to log and monitor abnormal behaviors. 3. ACTIVITIES AND DELIVERABLES 3.1 Activities and deliverables Deployment Phase Timeline Activities Milestones Completion Date Design \u0026amp; Prototype Weeks 1-3 - Design AWS architecture.\n- Collect RAG data.\n- Write SRS/SDS documentation. - Architecture Diagram \u0026amp; Cost Table.\n- RAG Pipeline Prototype.\n- Project Proposal Document. 12/10/-01/11/2025 Development (Core) Weeks 4-6 - Integrate Cognito.\n- Code Backend (Lambda).\n- Build Vector DB. - Auth System operational.\n- Complete API for VIP/Free.\n- RAG Knowledge Base ready. 02/11-22/11/2025 Deployment \u0026amp; QA Weeks 7-9 - Configure GitHub Actions.\n- Deploy to Prod environment.\n- Load Test \u0026amp; Pen Test. - System Live on AWS.\n- Test Report.\n- User Guide. 23/11-09/12/2025 3.2 Out of Scope The following items are outside the scope of the current project (MVP):\nMobile App development (iOS/Android). Real-time Voice Chat feature. 3.3 Operational Handover Process The current version is an MVP (Minimum Viable Product). To bring it to large-scale production, additional steps are required:\nFeature Expansion: Upgrade Lambda and Bedrock architecture to support React Native Mobile App or Voice Chat in the future. Operational Excellence: Set up more detailed AWS CloudWatch Alarms to monitor errors and latency. RAG Optimization: Fine-tune chunk sizes and retrieval strategies to reduce response latency. 4. COST ANALYSIS BY SERVICE Estimated cost for Demo environment (~5,000 requests/month) is $9.06/month.\nDetailed link: Cost Estimation Table\nLayer AWS Service Purpose Cost/Month Compute \u0026amp; API AWS Lambda, API Gateway, Amplify Backend Logic \u0026amp; Hosting ~$2.62 Data \u0026amp; Storage DynamoDB, S3 Chat History \u0026amp; RAG Storage ~$0.92 AI \u0026amp; Security Bedrock, Cognito, Parameter Store LLM Generation \u0026amp; Auth ~$2.65 Async \u0026amp; Monitoring SES, CloudWatch, EventBridge Email \u0026amp; Logging ~$2.88 Total $9.06 5. TEAM Overall Responsibility Name Title Description Email Nguyễn Gia Hưng Head of Architecture Solution Design and develop cloud-native and serverless platforms hunggia@amazon.com Stakeholders Name Title Description Email Đình Quang Sáng PQHDN Evaluation \u0026amp; Direction SangDQ6@fe.edu.vn Support Representatives Name Title Description Email Văn Hoàng Kha Cloud Security Engineer, Co-founded and led Viet-AWS Execute technical directions on Cloud Security and DevSecOps khavan.work@gmail.com Implementation Team Name Title Description Email Trần Phương Huyền Leader + Backend Dev Project Management + Backend Development tranphuonghuyen2005@gmail.com Nguyễn Lâm Anh AI Dev AI Development nguyenla110505@gmail.com Nguyễn Văn Linh AI Dev AI Development nguyenvanlinh.1710.it@gmail.com Bùi Nguyễn Tấn Khang Frontend Dev Frontend Development tankhang6a6@gmail.com 6. RESOURCES AND COST ESTIMATION Personnel Unit Price (Estimated personnel unit price based on student/academic project rates)\nResource / Role Responsibilities Unit Price (USD) / Hour Solution Architects - Overall architecture design (Hybrid Serverless, RAG Flow).\n- Selection of optimal AWS services (Bedrock, Lambda, Pinecone).\n- Ensuring non-functional requirements (security, latency, cost). 2.3 Software Engineers - Frontend development (Next.js/Amplify) and Cognito integration.\n- Backend API (Lambda) construction to handle user logic.\n- Chat History storage management (DynamoDB). 0.7 AI Engineers - Fine-tune Prompts for Bedrock for Tarot/Horoscope interpretation.\n- Build RAG flow: Document Chunking, Embeddings, Vector Search.\n- Evaluate accuracy and minimize AI hallucinations. 0.7 Estimated Hours \u0026amp; Cost by Phase (Effort estimation for 3 Iterations - 9 weeks)\nProject Phase Role Man-hours Cost (USD) Total Phase Cost Iter 3: Design \u0026amp; Prototype\n(Weeks 1-3) Solution Architect\nSoftware Engineer\nAI Engineer 30\n30\n30 $69.0\n$21.0\n$21.0 $111.0 Iter 4: Core Development\n(Weeks 4-6) Solution Architect\nSoftware Engineer\nAI Engineer 20\n80\n80 $46.0\n$56.0\n$56.0 $158.0 Iter 5: Deploy \u0026amp; QA\n(Weeks 7-9) Solution Architect\nSoftware Engineer\nAI Engineer 10\n60\n40 $23.0\n$42.0\n$28.0 $93.0 TOTAL 380 $362.0 Cost Contribution Allocation (Allocation of cost responsibility among stakeholders)\nParticipating Party Contribution Value (USD) Contribution Ratio of Total Partner (Team) $362.0 97.5% (Self-contributed personnel cost) AWS ~$9.06/month 2.5% (Estimated Cloud infrastructure cost) Customer $0 0% (Academic Project/MVP) 7. ACCEPTANCE The acceptance of the SorcererXStreme project will not only be based on feature completion but also on system stability and the quality of AI output.\n7.1. Deliverable Package The product is considered ready for acceptance when the project team provides all the following items:\nSource Code: Repository containing all Backend code (Lambda), Frontend code (Next.js), and Infrastructure as Code (Serverless Framework). Technical Documentation: Including SRS, SDS, API Documentation, and Deployment Guide. Live Environment: URL to access the stable functioning Demo environment on AWS. Quality Report: Test results (Test Cases) and accuracy assessment report of the RAG model. 7.2. Acceptance Process The process will take place in a 4-step sequence:\nLive Demo: The project team directly demos key User Flows: Registration -\u0026gt; Select Service -\u0026gt; Chat with AI -\u0026gt; Receive Results. User Acceptance Testing (UAT): The Customer/Mentor directly uses the system (03-05 days) to check the accuracy of spiritual knowledge and load capacity. Feedback \u0026amp; Fix: The project team commits to fixing Critical errors within 24-48 hours. Minor errors will be updated in subsequent patches. Completion Confirmation: The project is accepted when there are no critical errors and core features operate as committed. 7.3. Rejection Conditions The product will not be accepted if:\nDeployment Error: System Downtime or API error rate \u0026gt; 10%. Content Deviation: AI provides seriously misleading information or violates safety rules. Over Budget: Actual operating costs exceed the allowable threshold without reasonable explanation. "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.2-set-up/",
	"title": "Setup environment",
	"tags": [],
	"description": "",
	"content": "Step 1: Project Initialization \u0026amp; Library Installation Create the project directory and install all the necessary core dependencies.\n# Create the main directory for the Backend mkdir my-serverless-backend \u0026amp;\u0026amp; cd my-serverless-backend # Initialize a Node.js package npm init -y # Install Core Dependencies: Express, Prisma Client, Serverless-HTTP, etc. npm install express cors dotenv @prisma/client axios serverless-http # Install Development Dependencies: TypeScript, Types for Node/Express, Prisma CLI, Serverless-Offline npm install -D typescript @types/node @types/express serverless-offline prisma serverless-dotenv-plugin Step 2: TypeScript \u0026amp; Directory Structure Setup Configure TypeScript and create the standard directory structure.\nInitialize tsconfig.json: npx tsc --init Edit tsconfig.json: Open the tsconfig.json file and adjust the following settings to ensure modern Node.js source code and compatibility with Lambda: \u0026quot;target\u0026quot;: \u0026quot;ES2020\u0026quot; (or newer) \u0026quot;module\u0026quot;: \u0026quot;commonjs\u0026quot; \u0026quot;outDir\u0026quot;: \u0026quot;./dist\u0026quot; (Output directory for compiled code) \u0026quot;rootDir\u0026quot;: \u0026quot;./src\u0026quot; (Source code directory) \u0026quot;esModuleInterop\u0026quot;: true \u0026quot;strict\u0026quot;: true Create Directory Structure: mkdir src src/routes src/services src/controllers Step 3: Code Restructuring (Express -\u0026gt; Lambda) Since Lambda does not \u0026ldquo;listen\u0026rdquo; on a standard port, we use serverless-http to wrap Express.\nFile src/app.ts (Core Express App):\nimport express from \u0026#39;express\u0026#39;; import cors from \u0026#39;cors\u0026#39;; import routes from \u0026#39;./routes/index\u0026#39;; // Change to index if you use routes/index.ts const app = express(); // 1. Basic Middlewares app.use(cors({ origin: process.env.FRONTEND_URL || \u0026#39;*\u0026#39; })); app.use(express.json()); // 2. API Routing (Main Endpoint will be /api/...) app.use(\u0026#39;/api\u0026#39;, routes); // IMPORTANT: Do not use app.listen(), eliminate traditional web server logic. export default app; File src/handler.ts (Lambda Bridge):\nimport serverless from \u0026#34;serverless-http\u0026#34;; import app from \u0026#34;./app\u0026#34;; // Export the main handler that AWS Lambda will call export const handler = serverless(app); Step 4: Configure Prisma \u0026amp; NeonDB Connection To connect to NeonDB and ensure the Prisma Client works in the AWS Lambda Linux environment, the binaryTargets must be configured.\nCreate Schema File \u0026amp; Configure binaryTargets: npx prisma init Open the file prisma/schema.prisma and add the configuration: generator client {\rprovider = \u0026#34;prisma-client-js\u0026#34;\r// native: For the dev machine (Mac/Win)\r// rhel-openssl-3.0.x: For AWS Lambda (Node 20)\rbinaryTargets = [\u0026#34;native\u0026#34;, \u0026#34;rhel-openssl-3.0.x\u0026#34;] }\rdatasource db {\rprovider = \u0026#34;postgresql\u0026#34;\rurl = env(\u0026#34;DATABASE_URL\u0026#34;)\r}\r// ... model definitions (User, Partner, Reminder, etc.) Create Local .env file: Create the .env file and paste your NeonDB connection string into it. # Get the PostgreSQL connection string from the Neon Console DATABASE_URL=\u0026#34;postgresql://[user]:[password]@[endpoint]/[dbname]?sslmode=require\u0026#34; Generate Prisma Client: Run the following command to create the Prisma Client and download the necessary binaries. npx prisma generate Complete: Your local environment is now ready. You can compile the code (npm run build) and run local tests with serverless-offline.\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Master AWS Core Services: Gain deep understanding and hands-on experience with Compute (EC2), Storage (S3), and Identity Access Management (IAM). Project Technology Research: Investigate and evaluate Serverless Vector Database solutions (Pinecone, Neon) for future RAG implementation. Resource Preparation: Set up the development environment and gather/process raw data (Tarot, Horoscope). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study IAM concepts: Users, Groups, Roles, Policies for access management. - Practice: Configure Root MFA and create an Admin User for account security. 15/09/2025 15/09/2025 https://aws.amazon.com/iam/ 3 - Deep dive into S3: Storage Classes, Lifecycle Rules. - Collect and upload raw datasets (Tarot/Horoscope) to S3 for storage. 16/09/2025 16/09/2025 https://aws.amazon.com/s3/ 4 - Research EC2: Instance types (T2, T3\u0026hellip;), Security Groups, AMI. - Setup local Python environment with necessary AI libraries (LangChain, OpenAI) for logic testing. 17/09/2025 17/09/2025 https://aws.amazon.com/ec2/ 5 - AI Research: Compare RDS pgvector vs. Pinecone vs. Neon for vector storage. - Write a small Python script to test connection and data upsert to Pinecone (Free tier). 18/09/2025 18/09/2025 Pinecone \u0026amp; Neon Docs 6 - Research CI/CD concepts and why GitHub Actions is chosen. - 9:00 PM Team Meeting: Present Vector DB research findings and finalize input data structure. 19/09/2025 19/09/2025 Internal Team 7 - Optimize data processing scripts (cleaning, formatting to JSONL). - Summarize knowledge and write Week 2 Worklog. 20/09/2025 20/09/2025 Week 2 Achievements: AWS Infrastructure:\nSecured AWS account configuration (IAM, MFA) following best practices. Understood S3 mechanisms and successfully stored project raw data. Gained knowledge on EC2 instance launch and security configuration. Project Preparation (AI/Data):\nCompleted evaluation of Vector Database solutions. Selected Pinecone or Neon for the initial phase to optimize costs and leverage Serverless capabilities. Development environment is ready with all necessary libraries installed. Aligned with the team on the Data Schema for Tarot and Horoscope features. Process:\nGrasped the basic concepts of CI/CD, ready for automated pipeline implementation in the upcoming weeks. "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/",
	"title": "Backend Development",
	"tags": [],
	"description": "",
	"content": "Building and Automating with Serverless Backend (Serverless V4) This guide is a comprehensive document detailing the entire Backend development process for the SorcererXtreme AI project, from setting up the local development environment to completing the automated deployment workflow (CI/CD) on AWS.\n1. Context \u0026amp; Technical Challenges In the development of high-performance Serverless applications, we face several complex technical hurdles:\nFramework Transition: The transition from a traditional HTTP Framework like Express.js to the stateless environment of AWS Lambda requires using serverless-http and a change in source code structure. Database Management: Using a sophisticated ORM like Prisma on a Serverless platform necessitates techniques to optimize the deployment package size to under 250MB and requires downloading the correct Prisma Binary (rhel-openssl-3.0.x) for the Lambda Linux environment. Security: Ensuring that sensitive connection strings are never stored in the source code, but are securely managed using AWS SSM Parameter Store. 2. Core Value of the Constructed Architecture Our Backend architecture is designed to overcome these challenges, delivering key benefits:\nCost and Performance Optimization: Utilizing AWS Lambda and Serverless Framework V4 ensures you only pay for the time your code actually runs. Optimizing the Prisma package significantly reduces Cold Start time. Full Automation (CI/CD): Building the GitHub Actions workflow automates the entire development loop: Code -\u0026gt; Push -\u0026gt; Build -\u0026gt; Deploy, completely eliminating manual deployment steps and minimizing human error. Data Flexibility: Establishing a stable and secure connection to an external Database (NeonDB) demonstrates the capability for flexible integration in real-world projects. This guide will serve as a detailed, step-by-step map to help you master the entire process of modern Serverless Backend development.\nPrepare Set up environment Configure Serverless Framework Save secret key Automation CI/CD Connect microservices Set up email Development workflow "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.3-configure-serverless/",
	"title": "Configure Serverless Framework",
	"tags": [],
	"description": "",
	"content": "This is the most crucial step, where we define the AWS infrastructure (IaC) and optimize the deployment package for Lambda.\n1. The serverless.yml File This configuration file includes sections for SSM Security, Build/Package Optimization, and necessary IAM Roles.\n# serverless.yml org: your_organization_name service: my-serverless-backend # Ensure service name matches the directory provider: name: aws runtime: nodejs20.x region: ap-southeast-1 timeout: 29 # Maximum allowed by API Gateway (keep as is) memorySize: 512 # Lambda memory configuration (recommended for Prisma) # --- VPC Network Configuration (MANDATORY for NeonDB) --- # If NeonDB requires a fixed IP or you use an internal RDS, # Lambda needs VPC configuration to connect externally or within the VPC. # (Assumed for NeonDB Public Access, but VPC needed if using Private Subnet) # vpc: # securityGroupIds: [sg-xxxxxxxx] # subnetIds: [subnet-xxxxxx] # --- Environment Variables (Fetched from SSM) --- environment: # Retrieve secrets from AWS SSM Parameter Store (Encrypted) DATABASE_URL: ${ssm:/my-app/${self:provider.stage}/database_url} JWT_SECRET: ${ssm:/my-app/${self:provider.stage}/jwt_secret} FRONTEND_URL: ${ssm:/my-app/${self:provider.stage}/frontend_url, \u0026#39;http://localhost:3000\u0026#39;} # Add local fallback PRISMA_CLI_BINARY_TARGETS: rhel-openssl-3.0.x # Crucial for Prisma # --- IAM Permissions (Required) --- # Add necessary permissions for Lambda to read SSM and access other services iam: role: statements: - Effect: \u0026#39;Allow\u0026#39; Action: - \u0026#39;ssm:GetParameter\u0026#39; Resource: \u0026#39;arn:aws:ssm:${self:provider.region}:*:parameter/my-app/${self:provider.stage}/*\u0026#39; # --- DDoS/Billing Protection (Keep as is) --- apiGateway: usagePlan: quota: limit: 5000000 period: MONTH throttle: burstLimit: 200 rateLimit: 100 # --- Deployment Package Size Optimization --- build: esbuild: bundle: true minify: true sourcemap: false # Explicitly specify modules to exclude from the main package external: - \u0026#39;aws-sdk\u0026#39; - \u0026#39;@prisma/client/runtime/library\u0026#39; package: individually: true patterns: - \u0026#39;src/handler.js\u0026#39; - \u0026#39;src/app.js\u0026#39; - \u0026#39;src/**/*.js\u0026#39; - \u0026#39;dist/**/*.js\u0026#39; # Ensure compiled JS files are packaged - \u0026#39;package.json\u0026#39; - \u0026#39;node_modules/**\u0026#39; # --- Define Prisma Binary Files (Extremely important) --- # Only include the necessary Linux binary file to keep package \u0026lt; 250MB - \u0026#39;node_modules/.prisma/client/libquery_engine-rhel-openssl-3.0.x.so.node\u0026#39; - \u0026#39;node_modules/.prisma/client/schema.prisma\u0026#39; - \u0026#39;!./**\u0026#39; # Remove all unnecessary files after defining required patterns above - \u0026#39;!node_modules/aws-sdk/**\u0026#39; # Reduce size by excluding SDK already available in Lambda plugins: - serverless-offline - serverless-dotenv-plugin functions: api: handler: src/handler.handler events: - http: { path: /, method: ANY } - http: { path: /{proxy+}, method: ANY } 2. Required Steps A. Store Secrets in AWS SSM (Security) Before deployment, you must store sensitive values in the AWS SSM Parameter Store so the Serverless Framework can read them.\n# Command to store DATABASE_URL (SecureString) aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --value \u0026#34;postgresql://user:password@endpoint...\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # Repeat for JWT_SECRET and FRONTEND_URL B. Run Local Testing Use the serverless-offline plugin to run the API locally, connecting directly to NeonDB via the .env file.\n# Run local API on port 3000 (default) sls offline start C. First-time Deployment Once the code has been locally tested, you are ready to deploy the entire infrastructure to AWS.\n# Deploy all resources (Lambda, API Gateway, IAM) sls deploy "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS Time: Saturday, November 15, 2025, 8:30 – 12:00 Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City Role: Attendee (Learner)\nEvent Objectives Gain a comprehensive overview of the AI/ML landscape in Vietnam and the holistic AWS AI service ecosystem. Deep dive into Amazon SageMaker to understand the end-to-end MLOps process, from data preparation to model deployment. Master core Generative AI technologies on Amazon Bedrock, specifically Prompt Engineering and RAG. Understand the strategic shift from passive Chatbots to autonomous Agentic AI. Speakers Danh Hoang Hieu Nghi: Expert in Amazon Bedrock AgentCore and the Agent ecosystem. Dinh Le Hoang Anh: Demonstrated Pre-trained AI Services and real-world applications (AMZ Photo). Lam Tuan Kiet: Specialist in Generative AI, Prompt Engineering techniques, and RAG architecture. Key Highlights The event focused on three main pillars of the AWS AI Stack:\nAI Services (Pre-trained):\nIntroduction to ready-to-use services like Amazon Rekognition (image analysis), Transcribe (speech-to-text), and Polly (text-to-speech). Demo of AMZ Photo: A smart photo management app featuring face search and a multimodal chatbot built with the Pipecat framework. Generative AI with Amazon Bedrock:\nComparison of Foundation Models (Claude, Llama, Titan). Optimization techniques for Prompt Engineering: Zero-shot, Few-shot, and Chain-of-Thought (CoT). RAG (Retrieval-Augmented Generation) Architecture: Combining language models with enterprise data via Vector Databases. Agentic AI \u0026amp; AgentCore:\nThe evolution from Generative AI Assistants to Agentic AI (capable of autonomous execution). Introduction to AgentCore: Components such as Memory, Identity, Gateway, and Code Interpreter that enable Agents to operate at scale. Key Takeaways Prompt Engineering Mindset: Prompting is not just simple questioning but a natural language programming technique. Using Chain-of-Thought significantly improves the model\u0026rsquo;s ability to solve complex logic problems compared to standard querying. Standard RAG Process: Understanding the data flow (Data Ingestion -\u0026gt; Embeddings -\u0026gt; Vector Store -\u0026gt; Semantic Search) is crucial to solving AI hallucination issues. Power of Pre-trained Services: Building models from scratch is not always necessary. Leveraging existing APIs (like Rekognition, Textract) drastically reduces Time-to-market. Agent Architecture: grasping the distinction between a passive Chatbot and an active Agent capable of using Tools and accessing the Browser Viewer. Applying to Work Optimize Internal Chatbots: Immediately apply RAG architecture and Titan Text Embeddings V2 to upgrade the company\u0026rsquo;s technical documentation support bot, ensuring answers are accurate and based on actual data sources. Automate Document Processing: Propose using Amazon Textract combined with Comprehend to extract and classify information from invoices/contracts, replacing manual data entry processes. Research Agentic Workflows: Pilot a simple Agent using LangGraph or Amazon Bedrock Agents to automate the daily market news summarization process for the Business team. Personal Experience Attending the AWS Cloud Mastery Series at the AWS Vietnam office was professionally valuable for me:\nMr. Lam Tuan Kiet\u0026rsquo;s session was highly practical, especially the illustrative examples of Prompt Engineering. It helped me realize previous mistakes in interacting with LLMs and how to scientifically improve response quality. Mr. Dinh Le Hoang Anh\u0026rsquo;s demo with the AMZ Photo app was impressive. The integration of Multimodal capabilities and real-time processing demonstrated the immense potential of combining various AWS services. Mr. Danh Hoang Hieu Nghi\u0026rsquo;s topic on Agentic AI was very forward-looking. Although the AgentCore architecture is complex, it opens up a new perspective where AI is not just a support tool but can become a true \u0026ldquo;virtual employee.\u0026rdquo; Event Photos Summary: An in-depth workshop that balanced foundational theory with practical demos. Knowledge regarding RAG and Agentic AI will be the focus of my research in the near future.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.3-integration/",
	"title": "Integrating API Gateway &amp; Authentication",
	"tags": [],
	"description": "",
	"content": "1. Authentication Flow Before coding, let\u0026rsquo;s understand how Frontend communicates with Cognito and API Gateway:\nUser enters User/Pass. Cognito returns JWT Token (ID Token, Access Token). Frontend sends Request with Token in Authorization Header. API Gateway verifies Token. If valid -\u0026gt; Forward to Lambda. 2. Configure AWS Amplify Install aws-amplify library to connect Frontend with AWS services:\nnpm install aws-amplify @aws-amplify/ui-react Configure in src/app/layout.tsx:\n\u0026#39;use client\u0026#39;; import { Amplify } from \u0026#39;aws-amplify\u0026#39;; import config from \u0026#39;@/amplifyconfiguration.json\u0026#39;; Amplify.configure(config); 3. Integrate Amazon Cognito (Authentication) Use Authenticator component to create a secure login/signup flow:\nimport { Authenticator } from \u0026#39;@aws-amplify/ui-react\u0026#39;; export default function LoginPage() { return ( \u0026lt;Authenticator\u0026gt; {({ signOut, user }) =\u0026gt; ( \u0026lt;main\u0026gt; \u0026lt;h1\u0026gt;Welcome, {user?.username}\u0026lt;/h1\u0026gt; \u0026lt;button onClick={signOut}\u0026gt;Sign Out\u0026lt;/button\u0026gt; \u0026lt;/main\u0026gt; )} \u0026lt;/Authenticator\u0026gt; ); } 4. Custom Hook: useAuth (Best Practices) Instead of calling fetchAuthSession everywhere, create a Custom Hook to reuse authentication logic and token retrieval.\n// src/hooks/useAuth.ts import { fetchAuthSession } from \u0026#39;aws-amplify/auth\u0026#39;; import { useState, useEffect } from \u0026#39;react\u0026#39;; export function useAuth() { const [token, setToken] = useState\u0026lt;string | null\u0026gt;(null); useEffect(() =\u0026gt; { const getToken = async () =\u0026gt; { try { const session = await fetchAuthSession(); setToken(session.tokens?.idToken?.toString() || null); } catch (err) { console.error(\u0026#34;Error fetching auth session\u0026#34;, err); } }; getToken(); }, []); return { token }; } 5. API Call with Error Handling Handle errors professionally using try/catch/finally and display feedback to users.\n// src/services/api.ts export const chatWithAI = async (message: string, token: string) =\u0026gt; { try { const response = await fetch(`${process.env.NEXT_PUBLIC_API_URL}/chat`, { method: \u0026#39;POST\u0026#39;, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, \u0026#39;Authorization\u0026#39;: `Bearer ${token}` }, body: JSON.stringify({ message }) }); if (!response.ok) { if (response.status === 401) throw new Error(\u0026#34;Session expired\u0026#34;); if (response.status === 429) throw new Error(\u0026#34;Too many requests\u0026#34;); throw new Error(\u0026#34;System error\u0026#34;); } return await response.json(); } catch (error) { console.error(\u0026#34;API Error:\u0026#34;, error); throw error; // Throw error for UI to handle } }; My Experience Never expose API Keys! Once I accidentally committed a .env file containing API Keys to GitHub. AWS sent a warning email immediately. Solution: Always add .env to .gitignore. With Amplify, amplifyconfiguration.json is safe to be public as it only contains Resource IDs (like User Pool ID), not Secret Keys.\nVerification \u0026amp; Testing Test Case 1: Successful Login\nGo to Login page, enter created User/Pass. Click Sign In. Expected Result: Redirect to main page, display \u0026ldquo;Welcome, [Username]\u0026rdquo;. Test Case 2: Token Check\nOpen DevTools (F12) -\u0026gt; Network Tab. Send a Chat message. Find request sent to API Gateway. Check Header: Must have Authorization: Bearer eyJra... line (JWT Token). "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/5.4.3-storage-retrieval/",
	"title": "Storage &amp; Retrieval",
	"tags": [],
	"description": "",
	"content": "The system utilizes a Hybrid Retrieval mechanism (combining Vector Search and Key-Value Lookup). The core of this architecture is the use of Pinecone as the Long-term Memory for the AI.\n1. Vector Database: The Power of Pinecone Instead of self-managing infrastructure, the project selected Pinecone—a specialized Managed Vector Database. This component is crucial for enabling the AI to \u0026ldquo;understand\u0026rdquo; the semantics of Tarot or Horoscope inquiries, rather than relying solely on keyword matching.\nIndex Configuration Based on the Serverless architecture, the system utilizes Pinecone\u0026rsquo;s Serverless Index mode to optimize costs (paying only for data read/write, with no server maintenance fees).\nFigure 3.1: Actual Index Configuration on Pinecone Console.\nKey Technical Specifications:\nDimensions: 1024. This vector length is sufficient to encode the complex semantic nuances of metaphysical texts while remaining more optimal than 4096-dimensional models (too heavy) or 768-dimensional models (potentially lacking detail). Metric: Cosine Similarity. The system uses Cosine to measure the angle between two vectors. In semantic space, the smaller the angle between two vectors (Cosine approaching 1), the more similar their meanings. This metric is best suited for NLP (Natural Language Processing) tasks compared to Euclidean distance. Pod Type: Serverless (Automatically scales based on demand, no pre-provisioning required). Record Structure The true power of Pinecone lies in its ability to combine Vector Search with Metadata Filtering.\nFigure 3.2: Detail of a Vector Record including ID, Values, and Metadata.\nEach stored Record consists of three parts:\nID: Unique identifier (Hashed from original content) to prevent data duplication (Dedup). Values (Vector): An array of 1024 floating-point numbers, representing the meaning of the text segment. Metadata: This is the most critical part for increasing Accuracy. Instead of searching the \u0026ldquo;entire ocean,\u0026rdquo; the AI uses metadata to narrow the scope. Example: When a user asks about \u0026ldquo;Leo\u0026rsquo;s Love Life,\u0026rdquo; the system filters by category: \u0026quot;zodiac\u0026quot; and entity_name: \u0026quot;Leo\u0026quot; first, before finding the nearest vector. This completely eliminates the possibility of the AI retrieving information for the wrong sign. 2. Trade-off Analysis: Why Pinecone over AWS RDS (pgvector)? In the AWS ecosystem, the standard solution is often Amazon RDS for PostgreSQL with the pgvector extension. However, after rigorous trade-off consideration, Pinecone was selected.\nBelow is the detailed comparison analysis:\nCriteria Pinecone (Managed SaaS) Amazon RDS + pgvector (Self-Managed) Architecture Native Vector DB. Designed from the core specifically for high-speed vector storage and retrieval. Relational DB + Extension. A traditional relational database with vector processing capabilities \u0026ldquo;bolted on.\u0026rdquo; Operations (Ops) Zero Ops. No server management, no manual index tuning. Pure API calls. High Ops. Requires instance management, version updates, manual index configuration (IVFFlat/HNSW), and periodic database vacuuming. Latency Ultra-low (\u0026lt;50ms). Optimized for large-scale vector queries. Dependent on hardware configuration (CPU/RAM) and index tuning. Prone to slowness with large data if not well-optimized. Cost Pay-as-you-go. Billed based on stored vectors and R/W operations. Very cost-effective for project initiation. Fixed Hourly Cost. Must pay for instances running 24/7 even with zero usage (unless using Aurora Serverless v2, which is costly). Filtering Native Pre-filtering. Supports filtering metadata before vector search (Single-stage filtering), which is very powerful. Requires combining SQL WHERE clauses with vector search, which can sometimes impact performance if indexing is not precise. Conclusion: Given the current project scale and the requirement for rapid deployment (Time-to-market), Pinecone is the optimal choice due to its Serverless nature, allowing the team to focus on AI feature development rather than Database Administration (DBA) tasks.\n3. Why is DynamoDB still needed? Although Pinecone is powerful for Semantic Search, the system still requires DynamoDB for:\nExact Match Retrieval: Lambda Metaphysical needs to retrieve absolutely precise information based on IDs (e.g., the meaning of \u0026ldquo;The Fool\u0026rdquo; card when drawn, or specific star information in \u0026ldquo;Tu Vi\u0026rdquo;). DynamoDB performs this faster and cheaper than vector search. Latency Reduction: Offloads non-inferential tasks from the Vector DB. Raw Dataset Storage: Serves as a backup source and allows for quick metadata lookup without vector decoding. "
},
{
	"uri": "//localhost:1313/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 – AWS Weekly Roundup This blog provides a comprehensive summary of key AWS announcements and service updates released during the week. Highlights include the launch of new EC2 instance families—such as the storage-optimized I8ge powered by AWS Graviton4 and the general-purpose M8i and M8i-flex based on Intel Xeon 6. Additionally, AWS expanded IPv6 support for App Runner, Client VPN, and the RDS Data API.\nThe blog also covers improvements to EC2 Mac Dedicated Hosts and introduces new administrative controls for Amazon Q Developer, demonstrating AWS\u0026rsquo;s ongoing commitment to operational resilience and governance.\nBeyond technical updates, the roundup features notable community events, AWS Summits, and training resources, offering readers a well-rounded view of what is happening across the AWS ecosystem.\nBlog 2 – EU Cyber Resilience Act (CRA) \u0026amp; AWS IoT Compliance This article explores the European Union’s Cyber Resilience Act (CRA), a regulation that establishes mandatory cybersecurity requirements for products with digital elements. The blog explains the classification of digital products under CRA, essential security obligations for manufacturers, vulnerability reporting timelines, and lifecycle maintenance expectations.\nIt also discusses the practical implications for organizations building IoT products and outlines how AWS IoT services—such as IoT Core, Device Defender, Device Management, and EventBridge—can support compliance with CRA’s security-by-design principles.\nThrough real-world examples, including smart thermostat deployments, the blog demonstrates how enterprises can implement secure provisioning, access control, encryption, monitoring, patch management, and incident response workflows that align with CRA guidelines.\nBlog 3 – Amazon Disrupts APT29 Watering-Hole Campaign This security-focused article details Amazon’s investigation into a sophisticated watering-hole attack orchestrated by APT29, a nation-state threat actor. The blog describes how attackers compromised legitimate websites, injected obfuscated JavaScript, and selectively redirected users to malicious infrastructure designed to exploit Microsoft’s device code authentication flow.\nThe analysis highlights key adversarial techniques, including base64-encoded payloads, randomized redirection logic, persistent infrastructure rotation, and server-side redirects. It also shares indicators of compromise (IOCs) and emphasizes Amazon’s coordinated response with industry partners such as Cloudflare and Microsoft.\nThe blog concludes with actionable recommendations for both end-users and IT administrators to strengthen identity protection, adopt MFA, validate device authorization flows, and implement conditional access policies to reduce exposure to credential-harvesting campaigns.\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Master Networking: Understand and design secure Virtual Private Cloud (VPC) architectures with proper subnetting. Master Databases: Distinguish between and practice with RDS (SQL) and DynamoDB (NoSQL). AI/RAG Preparation: Deep dive into vector storage options, specifically comparing RDS (pgvector) vs. specialized solutions like Pinecone. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Study VPC theory: CIDR, Subnets (Public/Private), Route Tables, IGW vs NAT Gateway. - Understand Network ACLs vs Security Groups. 22/09/2025 22/09/2025 https://aws.amazon.com/vpc/ 3 - Practice: Design and deploy a Custom VPC with a 2-tier architecture (Public Web layer, Private DB layer). - 9:00 PM Team Meeting: Discuss basic network architecture and security for the upcoming project. 23/09/2025 23/09/2025 Internal Team 4 - Research RDS: Multi-AZ, Read Replicas, Backup strategies. - Research DynamoDB: NoSQL model, Partition Key, Sort Key. 24/09/2025 24/09/2025 https://aws.amazon.com/rds/ 5 - Practice: Launch an RDS Instance (PostgreSQL) and connect via EC2. - AI Research: Install and test the pgvector extension on PostgreSQL for vector storage. 25/09/2025 25/09/2025 GitHub pgvector 6 - Create DynamoDB tables and perform basic Put/Get Item operations. - 9:00 PM Team Meeting: Finalize Database strategy (Pinecone vs. RDS pgvector) balancing ease of use and cost. 26/09/2025 26/09/2025 Internal Team 7 - Summarize Networking \u0026amp; Database knowledge. - Write Week 3 Worklog. 27/09/2025 27/09/2025 Week 3 Achievements: Networking (VPC):\nSuccessfully built a VPC network topology including Public/Private Subnets, Route Tables, and NAT Gateways. Mastered Security Group configurations to secure traffic flow (e.g., restricting RDS access to only EC2 instances). Database \u0026amp; AI Data:\nSuccessfully deployed RDS (PostgreSQL) and DynamoDB instances. Key Achievement: Tested pgvector extension on PostgreSQL. Evaluated trade-offs between Pinecone (Serverless, specialized) and RDS + pgvector (Unified stack), preparing for the RAG implementation decision in the upcoming project phase. Teamwork:\nAligned with the team on the foundational network architecture (VPC) to be ready for application deployment starting next week (Week 4 - Project Kick-off). "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/",
	"title": "AI Development",
	"tags": [],
	"description": "",
	"content": "1. Introduction This document serves as a Technical Case Study detailing the process of building a multi-domain AI consulting system (Tarot, Numerology, Horoscopes, Astrology) based on Serverless architecture. The core objective is to solve the AI \u0026ldquo;hallucination\u0026rdquo; problem through a rigorous RAG (Retrieval-Augmented Generation) pipeline, ensuring responses strictly adhere to the verified Knowledge Base.\nProblem Statement: AI Challenges in Esoteric Domains Developing AI for the esoteric domain presents unique technical barriers that standard chatbot solutions often fail to address:\nAbstract Nature and Ambiguity: The data contains numerous Sino-Vietnamese terms and abstract concepts (e.g., card meanings changing based on spread positions, planetary influences shifting by birth hour). General-purpose LLMs often struggle to grasp this narrow context. Requirement for Absolute Accuracy (Zero Hallucination): Users seeking advice demand precision. AI fabricating knowledge (such as incorrect star positions or wrong number interpretations) is a critical risk that must be eliminated. Unstructured Data Processing: The input Knowledge Base is heterogeneous, requiring a complex ETL process to standardize data into Vector Embeddings without losing original semantic meaning. Cost Optimization Strategy A top priority for this project is optimizing operating expenses (OpEx) from Day 1. Instead of maintaining expensive server clusters 24/7, the system fully adopts a Pay-as-you-go model.\nThis architecture minimizes financial risk during the MVP (Minimum Viable Product) phase when user traffic is unpredictable. Below is the Unit Economics breakdown for key services:\nService Role Pricing Model Est. Unit Cost AWS Lambda Backend Logic Per request \u0026amp; compute duration ~$0.20 / 1M requests Pinecone Serverless Vector Database (Storage) Per Storage \u0026amp; Read Units ~$0.33 / GB / month Amazon Bedrock LLM Inference (Nova Pro) Per Input/Output Tokens Input: ~$0.0008 / 1k tokens\nOutput: ~$0.0032 / 1k tokens Cohere Embed Embedding Model (Multilingual) Per tokens processed ~$0.10 / 1M tokens DynamoDB Metadata \u0026amp; Cache Write/Read Capacity Units (On-demand) ~$1.25 / 1M Write Units Assessment: With this architecture, the system maintenance cost during idle states is virtually zero.\n2. AI Architecture Table of Contents System Architecture Data Strategy Storage \u0026amp; Retrieval AI Models \u0026amp; RAG CI/CD \u0026amp; Testing Lessons \u0026amp; Roadmap "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/5.4.4-models-rag/",
	"title": "AI Models &amp; RAG Pipeline",
	"tags": [],
	"description": "",
	"content": "The system leverages Amazon Bedrock as the centralized Unified API gateway, simplifying version management and credential security.\n1. Model Selection Analysis Embedding: Cohere Embed Multilingual v3 ID: cohere.embed-multilingual-v3 Why Cohere over Titan? Vietnamese/Sino-Vietnamese Nuances: Cohere v3 is trained on a massive multilingual dataset, allowing it to understand specific Sino-Vietnamese esoteric terms (e.g., \u0026ldquo;Thien Di\u0026rdquo;, \u0026ldquo;Phu The\u0026rdquo;) that Western-centric models often misinterpret. Input Type Parameter: Supports input_type=\u0026quot;search_query\u0026quot; (for user questions) and \u0026quot;search_document\u0026quot; (for database ingestion), optimizing the vector space for retrieval tasks. Dimension: 1024 dimensions - The \u0026ldquo;sweet spot\u0026rdquo; balancing semantic accuracy and Pinecone storage costs. Generative LLM: Amazon Nova Pro ID: amazon.nova-pro-v1:0 Reasoning Capability: Unlike simple summarization models, Nova Pro excels at logical chaining. Example: It can synthesize the meanings of 3 separate Tarot cards (Past - Present - Future) into a coherent cause-and-effect narrative. Inference Configuration: temperature: 0.3: Kept low to ensure the AI adheres strictly to the Knowledge Base, minimizing Hallucinations. topP: 0.9: Allows slight stylistic flexibility to make the advice sound natural and empathetic. 2. AWS Lambda Implementation Below is a Python (boto3) code snippet used within the AWS Lambda function to execute the RAG flow: Embed Query -\u0026gt; (Search Pinecone) -\u0026gt; Prompt Nova Pro.\nimport boto3 import json import os # Initialize Bedrock Runtime client bedrock = boto3.client( service_name=\u0026#39;bedrock-runtime\u0026#39;, region_name=\u0026#39;us-east-1\u0026#39; ) def generate_embedding(text): \u0026#34;\u0026#34;\u0026#34; Step 1: Convert user question to Vector \u0026#34;\u0026#34;\u0026#34; response = bedrock.invoke_model( modelId=\u0026#39;cohere.embed-multilingual-v3\u0026#39;, contentType=\u0026#39;application/json\u0026#39;, accept=\u0026#39;application/json\u0026#39;, body=json.dumps({ \u0026#34;texts\u0026#34;: [text], \u0026#34;input_type\u0026#34;: \u0026#34;search_query\u0026#34; # Optimized for queries }) ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;embeddings\u0026#39;][0] def query_llm(context_chunk, user_question, history): \u0026#34;\u0026#34;\u0026#34; Step 2: Send Context + Question to Amazon Nova Pro \u0026#34;\u0026#34;\u0026#34; # System Prompt: Define the Persona system_prompt = \u0026#34;\u0026#34;\u0026#34;You are a dedicated Tarot and Astrology expert. Answer the question based ONLY on the information provided in the \u0026lt;context\u0026gt; tags. If the information is insufficient, honestly state that you do not know. Do not fabricate facts.\u0026#34;\u0026#34;\u0026#34; # Construct Message Payload (Nova Pro structure) prompt_payload = { \u0026#34;system\u0026#34;: [{\u0026#34;text\u0026#34;: system_prompt}], \u0026#34;messages\u0026#34;: [ # Chat history can be injected here {\u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: f\u0026#34;Context: {context_chunk}\\n\\nQuestion: {user_question}\u0026#34;}]} ], \u0026#34;inferenceConfig\u0026#34;: { \u0026#34;temperature\u0026#34;: 0.3, # Reduce hallucinations \u0026#34;maxTokens\u0026#34;: 1000 } } response = bedrock.invoke_model( modelId=\u0026#39;amazon.nova-pro-v1:0\u0026#39;, body=json.dumps(prompt_payload) ) result = json.loads(response[\u0026#39;body\u0026#39;].read()) return result[\u0026#39;output\u0026#39;][\u0026#39;message\u0026#39;][\u0026#39;content\u0026#39;][0][\u0026#39;text\u0026#39;] "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.4-event4/",
	"title": "Event 4",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #2: DevOps on AWS Time: Monday, November 17, 2025, 8:30 – 17:00 Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City Role: Attendee (Learner)\nEvent Objectives Shift mindset from traditional system administration to DevOps Mindset and CI/CD culture. Master the AWS Developer Tools suite (CodeCommit, CodeBuild, CodeDeploy, CodePipeline) to automate development workflows. Approach Infrastructure as Code (IaC) techniques with CloudFormation and AWS CDK. Deep dive into Containerization technologies (Docker, ECR, ECS/EKS) and operating Microservices on AWS. Master Monitoring \u0026amp; Observability to supervise full-stack systems. Speakers AWS Technical Team: Solutions Architects (SA) specializing in DevOps and Modern Application Development. Experts in Containers and Serverless computing. Key Highlights The full-day event covered the journey from mindset to detailed practice:\nCI/CD Pipeline on AWS:\nFull workflow demo: Source Control (CodeCommit) -\u0026gt; Build \u0026amp; Test (CodeBuild) -\u0026gt; Deploy (CodeDeploy) -\u0026gt; Orchestration (CodePipeline). Safe deployment strategies like Blue/Green Deployment and Canary Release to minimize downtime risks. Infrastructure as Code (IaC):\nThe difference between manual \u0026ldquo;ClickOps\u0026rdquo; on the Console and defining infrastructure as code. Introduction to AWS CDK (Cloud Development Kit): Allows using familiar programming languages (Python, TS, Java) to provision infrastructure instead of writing lengthy YAML in CloudFormation. Container Services:\nReference architecture for Microservices using Amazon ECS and Fargate (Serverless compute for containers). The process of packaging Docker Images, storing on ECR, and automatic security scanning. Observability:\nBeyond basic logs and metrics, understanding systems through Distributed Tracing with AWS X-Ray. Key Takeaways DevOps Metrics: Understanding DORA metrics (Deployment Frequency, Lead Time for Changes\u0026hellip;) to measure team efficiency. Git Strategies: Knowing how to choose the right branching model (GitFlow vs. Trunk-based development) for different project scales. Container Workflow: Mastering the lifecycle of a container from writing a Dockerfile to running in a Production environment with Auto Scaling. Drift Detection: How to detect and handle discrepancies between actual Cloud configuration and the defined IaC code. Applying to Work Automate Build \u0026amp; Deploy: Propose shifting from manual deployment (copy-pasting files) to using AWS CodePipeline combined with existing Github Actions to reduce human error. Implement IaC: Start using AWS CDK for new projects to easily manage infrastructure versions and reuse code (Constructs). Improve Monitoring: Integrate CloudWatch Alarms and X-Ray into critical services to receive early warnings before customers report errors. Personal Experience Attending a full day on DevOps helped me \u0026ldquo;crack\u0026rdquo; many aspects of professional software processes:\nThe CI/CD Pipeline demo was visually satisfying. Just pushing code to the repo triggered automated tests, docker image builds, and server updates without manual intervention. I really enjoyed the session on AWS CDK. I used to be intimidated by thousand-line CloudFormation templates, but with CDK, defining VPCs, Subnets, and EC2s takes just a few lines of clean, logical Python code. The ECS architecture drawn on the whiteboard (as captured in the photos) helped me clearly visualize the request flow from Internet Gateway -\u0026gt; ALB -\u0026gt; Target Group -\u0026gt; Container, and how to separate Public/Private subnets for security. Event Photos Summary: \u0026ldquo;Automation is King.\u0026rdquo; The event made me realize that any task repeated more than twice should be automated. DevOps is not just tools; it\u0026rsquo;s a culture of efficiency.\n"
},
{
	"uri": "//localhost:1313/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": "During my internship, I participated in five events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: Vietnam Cloud Day 2025 : Ho Chi Minh City Connect Edition for Builders\nDate \u0026amp; Time: Thursday, 18 September 2025, 9:00 – 17:30\nLocation: 36th floor, 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole: Attendee\nDescription: A deep-dive tech event for Builders, focusing on strategic trends such as Agentic AI, Data Foundation solutions, and the AI-Driven Development Lifecycle (AI-DLC). It also covered essential security standards for Generative AI.\nOutcomes: Gained insights into the core mindset and architecture of Agentic AI; deeply understood the AI-DLC process to shift from coding to architecting and reviewing; updated key knowledge on multi-layer security for GenAI applications.\nEvent 2 Event Name: Workshop Data Science on AWS\nDate \u0026amp; Time: 16 October 2025, 9:30 – 11:45\nLocation: Hall A - FPT University HCMC (FPTU HCMC)\nRole: Attendee\nDescription: A practical workshop on Data Science on AWS platform, focusing on the AI/ML ecosystem stack, key AI Services (Vision, Speech, Text), and the model training workflow using Amazon SageMaker.\nOutcomes: Understood the big picture of AWS AI/ML Stack; learned how to quickly integrate AI Services (Rekognition, Polly, Lex) into applications; grasped the Feature Engineering process and cost optimization strategies when running models on the Cloud.\nEvent 3 Event Name: AWS Cloud Mastery Series #1: AI/ML/GenAI on AWS\nDate \u0026amp; Time: Saturday, 15 November 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: An intensive series on Generative AI, focusing on Prompt Engineering techniques, RAG (Retrieval-Augmented Generation) architecture, and building AI Agents with Amazon Bedrock.\nOutcomes: Deeply understood the differences between Foundation Models (Claude, Llama, Titan); mastered Chain-of-Thought prompting; learned the Bedrock Agent architecture to build smart chatbots capable of using tools.\nEvent 4 Event Name: AWS Cloud Mastery Series #2: DevOps on AWS\nDate \u0026amp; Time: Monday, 17 November 2025, 8:30 – 17:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: A full-day deep dive into DevOps culture and tools on AWS. Content covered building automated CI/CD pipelines (CodePipeline), managing infrastructure as code (IaC) with AWS CDK, Containerization techniques (ECS/EKS), and comprehensive system Observability.\nOutcomes: Shifted mindset from manual administration to \u0026ldquo;Automation is King\u0026rdquo;; mastered the AWS Developer Tools suite to eliminate manual operations; learned how to use AWS CDK to define infrastructure using programming languages; gained a clear understanding of Microservices architecture and safe deployment strategies (Blue/Green).\nEvent 5 Event Name: AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar\nDate \u0026amp; Time: Saturday, 29 November 2025, 8:30 – 12:00\nLocation: AWS Vietnam Office, Bitexco Financial Tower, HCMC\nRole: Attendee\nDescription: A workshop dedicated to the Security Pillar, covering Zero Trust principles, Modern Identity and Access Management (IAM), Detection-as-Code, and Automated Incident Response.\nOutcomes: Adopted the \u0026ldquo;No ClickOps\u0026rdquo; mindset in security (configuration must be code); deeply understood the Shared Responsibility Model; learned to use EventBridge and Lambda to build systems that automatically remediate vulnerabilities upon detection; gained confidence in designing multi-layered secure network architectures.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.4-deployment/",
	"title": "Setting up CI/CD Pipeline with AWS Amplify",
	"tags": [],
	"description": "",
	"content": "1. CI/CD Process We will set up a fully automated process as follows:\nSource: Developer pushes code to GitHub. Build: Amplify automatically detects changes, installs dependencies, and builds Next.js. Deploy: Pushes built code to global Hosting system. Verify: Checks website health (Health Check). 2. Connect Repository Push code to Git Repository (GitHub/GitLab/CodeCommit). In AWS Amplify Console, select Host web app. Connect to the Repository containing Frontend source code. 3. Build Configuration (Build Specification) Amplify uses amplify.yml to define build steps.\nversion: 1 frontend: phases: preBuild: commands: - npm ci build: commands: - npm run build artifacts: baseDirectory: .next files: - \u0026#39;**/*\u0026#39; cache: paths: - node_modules/**/* 4. Environment Variables Management Never hard-code sensitive or environment-specific values (like API URLs) in your code. Use Environment Variables in Amplify.\nGo to App settings \u0026gt; Environment variables. Add variable: Key: NEXT_PUBLIC_API_URL Value: https://xyz.execute-api.us-east-1.amazonaws.com/prod Access in Next.js code via process.env.NEXT_PUBLIC_API_URL. Note: Variables starting with NEXT_PUBLIC_ will be embedded into the Frontend code by Next.js at build time.\n5. Branch Previews (Pull Request Previews) This feature is incredibly useful for team collaboration.\nWhen you create a Pull Request (PR) on GitHub, Amplify automatically creates a temporary Preview environment (with a unique URL). Team Leaders can visit that URL to review new features before Merging into the main branch. After Merging, the Preview environment is automatically deleted. To enable: Go to App settings \u0026gt; Previews \u0026gt; Enable previews.\nMy Experience Build Error on Linux vs Windows My machine uses Windows (case-insensitive), but Amplify runs Linux (case-sensitive). Once I imported Component.tsx but the file was named component.tsx. It ran fine locally, but failed on Amplify with \u0026ldquo;File not found\u0026rdquo;. Lesson: Always name files consistently (PascalCase for Components, camelCase for utils) and double-check when renaming files.\nVerification \u0026amp; Testing Test Case 1: Trigger Build\nEdit a small text line in page.tsx. Commit and Push to GitHub: git push origin main. Go to Amplify Console. Expected Result: See status change to \u0026ldquo;Provisioning\u0026rdquo; -\u0026gt; \u0026ldquo;Building\u0026rdquo; -\u0026gt; \u0026ldquo;Deploying\u0026rdquo;. Test Case 2: Check Logs\nClick on the running build. Open Frontend tab. Expected Result: See green logs (Success) in steps. 2024-05-20T10:00:00.000Z [INFO]: # Executing command: npm run build\r...\r2024-05-20T10:01:00.000Z [INFO]: Compiled successfully "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.4-aws-ssm/",
	"title": "Storing Sensitive Environment Variables",
	"tags": [],
	"description": "",
	"content": "Storing sensitive environment variables (DATABASE_URL, JWT_SECRET) in AWS Systems Manager (SSM) Parameter Store is the security standard for Serverless applications.\n1. Preparation In your serverless.yml, you used the variable ${self:provider.stage} (defaulting to dev if not specified). To be consistent with that configuration, we should define the Key in the format /my-app/\u0026lt;stage\u0026gt;/\u0026lt;key\u0026gt;.\n2. Run CLI Commands to Store Keys You need to run these commands via the AWS CLI after configuring access permissions.\n# NOTE: Replace \u0026lt;YOUR_VALUE\u0026gt; with your actual value. # We will use the default stage \u0026#34;dev\u0026#34; for the workshop environment. # --- 1. Store NeonDB Connection String --- # Use \u0026#34;SecureString\u0026#34; to encrypt the data. aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --value \u0026#34;postgresql://user:pass@ep-xyz.aws.neon.tech/neondb?sslmode=require\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # --- 2. Store JWT Secret --- aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/jwt_secret\u0026#34; \\ --value \u0026#34;a_very_long_and_secure_jwt_key_314159\u0026#34; \\ --type \u0026#34;SecureString\u0026#34; \\ --overwrite # --- 3. Store Frontend URL (If necessary for CORS) --- aws ssm put-parameter \\ --name \u0026#34;/my-app/dev/frontend_url\u0026#34; \\ --value \u0026#34;https://your-amplify-app.amplifyapp.com\u0026#34; \\ --type \u0026#34;String\u0026#34; \\ --overwrite 3. Verification After storing, you can run the following command to check if the Parameter has been saved correctly and if the Serverless Framework can access it:\n# Command to check and decrypt the value (requires SSM Read permission) aws ssm get-parameter \\ --name \u0026#34;/my-app/dev/database_url\u0026#34; \\ --with-decryption Important Note: In serverless.yml, you defined: DATABASE_URL: ${ssm:/my-app/prod/database_url}. To synchronize with the command above, you need to change prod to dev in serverless.yml if you are deploying with the default stage, or run the CLI commands with the prod stage if you wish to keep the serverless.yml file as is.\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Security \u0026amp; Monitoring: Secure infrastructure and set up CloudWatch monitoring to manage costs and performance from day one. Project Kick-off: Finalize feature scope, assign roles, and establish team workflows. AI Prototyping: Build a local AI prototype to validate RAG logic and Prompt Engineering for Tarot/Chatbot features. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure AWS CloudWatch Alarms for Billing (budget control) and EC2/RDS performance monitoring. - Enable CloudTrail for account auditing and security logging. 29/09/2025 29/09/2025 https://aws.amazon.com/cloudwatch/ 3 - Prepare detailed feature list: Tarot (1-card/3-card), Horoscope, Numerology. - 9:00 PM Team Meeting: Project Kick-off, finalize Tech stack (BE: Lambda, DB: Pinecone/RDS), and Gitflow process. 30/09/2025 30/09/2025 Internal Team 4 - AI Coding: Build a local Python script to simulate Tarot drawing and interpretation generation (using OpenAI API + Prompt templates). - Test Prompt Engineering techniques to define the AI Persona (\u0026ldquo;Fortune Teller\u0026rdquo;). 01/10/2025 01/10/2025 prompts.py 5 - RAG Coding: Integrate Pinecone into the local script to retrieve card meanings from vectorized data. - Evaluate response accuracy with and without RAG. 02/10/2025 02/10/2025 Pinecone Docs 6 - Design Chatbot Logic: Define guardrails to restrict the bot to spiritual topics and refuse out-of-scope questions. - 9:00 PM Team Meeting: Assign tasks on Jira/Trello. Officially take on the AI Lead role. 03/10/2025 03/10/2025 Internal Team 7 - Document the AI Logic Flow for the Backend team to understand integration points. - Write Week 4 Worklog. 04/10/2025 04/10/2025 Week 4 Achievements: Infrastructure \u0026amp; Management:\nEstablished monitoring dashboards and billing alarms on AWS. Finalized workflows and feature list for the MVP (Minimum Viable Product). AI \u0026amp; Application (Local):\nSuccessfully built a Local AI Prototype: Input: User question + Drawn cards. Process: RAG (Pinecone) retrieves context -\u0026gt; LLM (OpenAI) generates advice. Output: Spiritual, empathetic responses. Defined clear boundaries for the Chatbot: Restricted it from answering questions that overlap with specific reading features (e.g., Chatbot guides users to the Tarot feature instead of doing the reading itself). Teamwork:\nOfficially accepted the AI Lead role and assigned tasks for the first Sprint. "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.5-cicd/",
	"title": "Automation CI/CD",
	"tags": [],
	"description": "",
	"content": "The objective of this section is to establish a process so that every time code is pushed to the main branch, your entire Serverless architecture will be automatically Built, Tested, and Deployed to AWS without manual intervention from a local machine.\n1. Preparing Secrets on the GitHub Repository This is the most crucial step for granting GitHub Actions access to AWS.\nSecret Name Role Notes and Action AWS_ACCESS_KEY_ID AWS Access Key. VERY IMPORTANT: Use an IAM User with minimum necessary permissions (not Admin) to only allow creation/updating of Lambda, API Gateway resources, and reading SSM. AWS_SECRET_ACCESS_KEY Corresponding Secret Key. (Keep as is) SERVERLESS_ACCESS_KEY Serverless Dashboard Key. Only needed if you use Serverless Dashboard management features. If not, this can be omitted. 2. Workflow File .github/workflows/deploy.yml This file defines the steps that GitHub Actions will execute. We will add the Build step and omit environment variables unnecessary for the deploy command.\nname: Deploy Backend CI/CD via Serverless Framework on: push: branches: [ main ] # Triggers when pushing to the main branch workflow_dispatch: # Allows manual triggering from the GitHub UI jobs: deploy: runs-on: ubuntu-latest # Grant permissions for GitHub Actions permissions: id-token: write contents: read steps: - uses: actions/checkout@v4 # 1. Get source code - name: Setup Node 20 uses: actions/setup-node@v3 with: node-version: \u0026#39;20\u0026#39; cache: \u0026#39;npm\u0026#39; # Enable cache to speed up installation - name: Install Dependencies run: npm ci # Use npm ci to ensure consistency (from package-lock.json) # --- PRISMA \u0026amp; CODE BUILD CONFIGURATION --- - name: Generate Prisma Client (download Linux binary) # Download the rhel-openssl-3.0.x binary (necessary for Lambda) run: npx prisma generate - name: Build TypeScript Code (tsc -\u0026gt; dist) # Run the compilation command (assumes a \u0026#34;build\u0026#34;: \u0026#34;tsc\u0026#34; script in package.json) run: npm run build # --- AWS CONFIGURATION --- - name: Configure AWS Credentials uses: aws-actions/configure-aws-credentials@v4 with: aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }} aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }} aws-region: ap-southeast-1 # --- FINAL DEPLOYMENT --- - name: Deploy Serverless # Use npx to call the serverless CLI run: npx serverless deploy env: # Explicitly specify the deployment stage. SLS_STAGE: dev # [Note]: The DATABASE_URL variable is already handled by placeholder or SSM DATABASE_URL: \u0026#34;placeholder\u0026#34; 3. CI/CD Workflow Summary Code Commit: You develop code and perform a git push to GitHub. Trigger: GitHub Actions automatically triggers the deploy.yml workflow. Build \u0026amp; Package: The Ubuntu Runner installs dependencies, runs npx prisma generate (downloading the Linux binary), and compiles the code. AWS Authentication: aws-actions/configure-aws-credentials logs into AWS using your Secret Keys. Provisioning: The npx serverless deploy command reads serverless.yml, connects to SSM Parameter Store to fetch secret keys, and instructs CloudFormation to build/update the entire Lambda and API Gateway infrastructure. "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/5.4.5-cicd/",
	"title": "CI/CD &amp; Testing",
	"tags": [],
	"description": "",
	"content": "The system implements a fully automated CI/CD pipeline via GitHub Actions, ensuring that every line of code is rigorously tested (Test) before being deployed to the AWS Lambda environment.\n1. CI Mechanism – Continuous Integration Whenever new code is Pushed or a Pull Request is opened against the main branch, the CI workflow is automatically triggered to verify logic integrity.\nWorkflow: [Code Commit] ➔ [GitHub Actions Trigger] ➔ [Runner Ubuntu Start] ➔ [Setup Python Env] ➔ [Run Pytest] ➔ [Result: ✅ Pass / ❌ Fail]\nMocking Strategy: All Unit Tests utilize unittest.mock to simulate AWS services (S3, DynamoDB, Bedrock). This ensures: Zero Cost: Testing does not incur AWS API fees. High Speed: Tests complete in seconds rather than waiting for network latency. Safety: Production data is never overwritten or deleted during testing. 2. CD Mechanism – Continuous Deployment The CD pipeline is activated to package and deploy code to AWS only when the CI process returns a Success (✅) result.\nWorkflow: [CI Success] ➔ [Install Binary Libs] ➔ [Package Zip 📦] ➔ [AWS CLI Upload 🚀] ➔ [Lambda Live Update]\nNote on Deployment Package: A major challenge with AWS Lambda is the compatibility of C-extension libraries (like numpy for calculations). The system resolves this by installing and building libraries with the --platform manylinux2014_x86_64 flag directly within the CD pipeline to ensure absolute compatibility with the Amazon Linux environment.\n3. Unit Testing Strategy The testing framework covers three logic layers: Input Validation, Happy Path, and Error Handling.\nChatbot Lambda (sorcererxstreme-chatbot) Input Validation: Case 1 (Missing Session): Missing sessionId → Returns 400 Bad Request. Case 2 (Missing Question): Empty question content → Returns 400. Case 3 (Invalid JSON): Malformed JSON body → Returns Parse Error. Happy Path: Case 4: Simulates full RAG flow: Load History → Query Pinecone → Call Bedrock → Save Chat. Asserts that AI returns a 200 OK response. Embedding Lambda (sorcererxstreme-embedding) Logic \u0026amp; Utilities: Case 1 (Flatten Contexts): Verifies JSON flattening. Input {\u0026quot;hobbies\u0026quot;: [\u0026quot;code\u0026quot;, \u0026quot;read\u0026quot;]} must become string \u0026quot;hobbies: code, read\u0026quot;. Case 2 (Bedrock Fail): Simulates Bedrock network error. get_embedding must return None to avoid crashing the Batch process. Integration Flow: Case 3 (Happy Flow): Simulates reading JSONL from S3 → Embedding 2 items → Writing to DynamoDB \u0026amp; Pinecone. Verifies API call counts match item counts. Case 4 (S3 Error): Simulates S3 file not found → System reports 500 (Internal Error). Metaphysical Lambda (sorcererxstreme-metaphysical) This is the most complex function with domain-based routing logic.\nInput Validation: Case 1 (Invalid Domain): Sending domain: \u0026quot;bitcoin\u0026quot; → Returns 400 (Only Tarot, Horoscope, Numerology, Astrology supported). Case 2 (Missing Context): Selecting \u0026ldquo;Horoscope\u0026rdquo; but missing birth_date → Returns input request. Case 3 (Empty Tarot): Selecting \u0026ldquo;Tarot\u0026rdquo; but cards_drawn array is empty → Warns user to select cards. Domain Logic: Case 4 (Tarot Flow): Mocks DynamoDB returning \u0026ldquo;The Sun\u0026rdquo; meaning. Verifies AI receives the correct context for analysis. Case 5 (Horoscope Flow): Verifies integration with lasotuvi library. Mocks \u0026ldquo;Thien Ban\u0026rdquo; object to ensure AI receives \u0026ldquo;Element: Earth\u0026rdquo; (Mệnh Thổ) data. Case 6 (Graceful Degradation): When Bedrock is throttled, the system returns a user-friendly message instead of crashing. "
},
{
	"uri": "//localhost:1313/4-eventparticipated/4.5-event5/",
	"title": "Event 5",
	"tags": [],
	"description": "",
	"content": "AWS Cloud Mastery Series #3: AWS Well-Architected Security Pillar Time: Saturday, November 29, 2025, 8:30 – 12:00 Location: AWS Vietnam Office, Bitexco Financial Tower, District 1, Ho Chi Minh City Role: Attendee (Learner)\nEvent Objectives Deeply understand the Security Pillar of the AWS Well-Architected Framework, shifting mindset from Perimeter Security to Zero Trust. Master Modern IAM architecture for managing identity and access at scale. Learn to build an \u0026ldquo;immune system\u0026rdquo; with Detection-as-Code and Automated Remediation. Practice Infrastructure Protection and Data Protection strategies against common attack vectors. Speakers Tran Duc Anh - Cloud Security Engineer Trainee, AWS Cloud Club Captain SGU. Nguyen Tuan Thanh - Cloud Engineer Trainee. Nguyen Do Thanh Dat - Cloud Engineer Trainee. (Featuring the technical team from AWS Cloud Clubs and First Cloud Journey) Key Highlights The workshop deep-dived into the 5 core pillars of security on AWS:\nIdentity \u0026amp; Access Management (IAM):\n\u0026ldquo;Kill long-lived credentials\u0026rdquo; principle: Replace permanent Access Keys with IAM Roles and Temporary Tokens. Using IAM Access Analyzer to detect overly permissive policies (like Principal: *). Demo on configuring SSO (Single Sign-On) for centralized access management across multiple AWS accounts. Detection \u0026amp; Continuous Monitoring:\nImplementing Detection-as-Code: Using CloudFormation/Terraform to enable GuardDuty and Security Hub organization-wide instead of manual clicking. Runtime Monitoring: GuardDuty Agent installed deep within EC2/EKS to detect strange processes (process injection) or anomalous file access. Infrastructure Protection:\nDefense in Depth strategy: Layering WAF, Shield at the Edge -\u0026gt; Network Firewall in VPC -\u0026gt; Security Groups at the Instance level. Clearly distinguishing the roles of Security Groups (Stateful) and NACLs (Stateless) in defending against DDoS or Lateral Movement attacks. Incident Response (IR):\nAutomating the incident response process: CloudTrail logs -\u0026gt; EventBridge filter -\u0026gt; Lambda function to automatically isolate malware-infected EC2s or rotate exposed credentials. Key Takeaways \u0026ldquo;No ClickOps\u0026rdquo; Mindset: In security, manual configuration is the enemy. Every security rule, from Security Groups to GuardDuty detectors, must be defined by Code (IaC) and version controlled. Shared Responsibility Model: Clearly understanding what AWS handles (Security of the Cloud) and what we must handle (Security in the Cloud), especially regarding Data Encryption (at rest/in transit). Power of EventBridge: It\u0026rsquo;s not just a message bus but the \u0026ldquo;central nervous system\u0026rdquo; connecting Detection to Remediation in real-time. Micro-segmentation: Never trust the internal network. Segmenting VPCs into public/private subnets and tightening them with Security Groups is mandatory. Applying to Work Immediate IAM Review: Audit all IAM Users in company projects, delete unused Access Keys, and enable MFA for 100% of accounts, especially Root accounts. Deploy GuardDuty: Propose enabling GuardDuty at the Organization level to instantly detect basic threats (like crypto mining, port scanning) with minimal configuration effort. Secret Management: Stop hardcoding passwords in code or .env files immediately. Switch to AWS Secrets Manager and configure Automatic Rotation for DB credentials. Personal Experience Honestly, it was a \u0026ldquo;brain-stretching\u0026rdquo; but incredibly high-quality morning. Although the speakers were Trainees/Students, the knowledge was deep and battle-tested:\nI was very impressed with the \u0026ldquo;Prevention - Nobody has time for incidents\u0026rdquo; slide. The quote \u0026ldquo;Public buckets = your data on the evening news\u0026rdquo; was both funny and poignant regarding the importance of blocking S3 public access. The Automated Remediation demo opened my eyes. I used to think Incident Response required someone monitoring screens 24/7, but it turns out Lambda can be written to \u0026ldquo;neutralize\u0026rdquo; threats the moment they appear. The Network Attack Vectors slide visually mapped out a Hacker\u0026rsquo;s path from the Internet through protection layers, helping me clearly visualize a secure network architecture. Event Photos Summary: Security is not a blocker, but a key enabler for businesses to move faster and safer. This event gave me significant confidence to propose security solutions for upcoming projects.\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.5-advanced-deployment/",
	"title": "Optimization &amp; Advanced Deployment",
	"tags": [],
	"description": "",
	"content": "After successfully deploying SorcererXtreme to the Internet, our work is not done. To make the product truly \u0026ldquo;Production-Ready\u0026rdquo;, we need to perform advanced tuning.\n1. Custom Domain By default, AWS Amplify provides a long and hard-to-remember URL (e.g., main.d12345.amplifyapp.com). Setting up a custom domain not only makes the app look professional but also improves trust.\nProcess:\nPurchase Domain: You can buy directly on Amazon Route 53 or other providers (Namecheap, GoDaddy). Configure in Amplify: Go to App settings \u0026gt; Domain management. Click Add domain and enter your domain (e.g., sorcererxtreme.vn). DNS Verification: If on Route 53: Amplify auto-configures (Zero-config). If external: Amplify provides a CNAME record for you to add to your provider\u0026rsquo;s DNS manager. SSL/TLS: Amplify automatically issues and manages free SSL certificates, ensuring the green padlock (HTTPS). 2. Environment Variables Management During development, we often use .env.local files to store sensitive Keys. However, these files are not pushed to Git when deploying.\nWhy important?\nSecurity: Hides API keys (like API Gateway Endpoint, Cognito User Pool ID) from public source code. Flexibility: Easily switch configs between Staging and Production environments without changing code. Setup:\nAccess Amplify Console \u0026gt; Select App \u0026gt; Environment variables. Enter corresponding Keys (e.g., NEXT_PUBLIC_API_URL, NEXT_PUBLIC_USER_POOL_ID). Trigger the Build process again for changes to take effect. 3. SEO Optimization for Next.js (Search Engine Optimization) For a B2C user-facing app like Tarot reading, appearing on Google is vital. Next.js (App Router) strongly supports SEO via the Metadata API.\nDynamic Metadata: Instead of static titles, create dynamic titles based on the Tarot card the user draws:\n// app/tarot/[cardId]/page.tsx export async function generateMetadata({ params }) { const card = await getTarotCard(params.cardId); return { title: `Meaning of ${card.name} | SorcererXtreme`, description: `Discover the cosmic message from ${card.name}...`, openGraph: { images: [card.imageUrl], // Image shown when shared on Facebook }, } } 4. Monitoring \u0026amp; Analytics You cannot improve what you do not measure. Use the Monitoring Tab in Amplify to track:\nIncoming Traffic: Number of visitors in real-time. Data Transfer: Bandwidth usage. Error Rate (4XX/5XX): Detect immediately if APIs fail or links break. Access Logs: Download access logs to analyze user behavior (origin country, browser used). Front-end Tip: Always check your Lighthouse score (in Chrome DevTools) after every deploy. A beautiful app that loads slowly will lose users instantly. Optimize images (use WebP/AVIF format) and lazy-load heavy components.\n"
},
{
	"uri": "//localhost:1313/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: AWS Scaling \u0026amp; Load Balancing: Master the configuration of Application Load Balancer (ALB) and Auto Scaling Groups (ASG) for high availability. Data Engineering: Finalize the Data Ingestion Pipeline and process raw datasets into vector embeddings. RAG Optimization: Refine data chunking strategies and update advanced prompt templates for specific domains (Love, Career). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - AWS Practice: Configure ALB with Target Groups and define Launch Templates for ASG. - Perform stress tests to validate auto-scaling logic based on CPU utilization. 06/10/2025 06/10/2025 https://aws.amazon.com/autoscaling/ 3 - Batch process dataset_v4.jsonl to generate vector embeddings. - 9:00 PM Team Meeting: Review estimated costs for Pinecone/Neon with large data ingestion. 07/10/2025 07/10/2025 Internal Team 4 - Optimize Chunking Strategy: Test optimal text segment lengths for RAG retrieval using dataset_v3.jsonl. - Review logic in lambda_functions.py in preparation for cloud migration. 08/10/2025 08/10/2025 Pinecone Docs 5 - Refine Prompt System: Update prompts_master.py with specialized scenarios for Love and Career readings. - Write Python scripts to automate embedding updates for new data. 09/10/2025 09/10/2025 prompts_master.py 6 - Deep dive into Metadata Filtering in Pinecone to improve RAG accuracy (e.g., filter by \u0026lsquo;Tarot\u0026rsquo; vs \u0026lsquo;Horoscope\u0026rsquo;). - 9:00 PM Team Meeting: Finalize the Vector Storage strategy. 10/10/2025 10/10/2025 Internal Team 7 - Summarize Scaling knowledge. - Write Week 5 Worklog. 11/10/2025 11/10/2025 Week 5 Achievements: AWS Infrastructure:\nValidated Auto Scaling logic to ensure system resilience under high traffic loads. Gained practical experience in traffic distribution using Application Load Balancer. Data \u0026amp; AI (Project):\nSuccessfully indexed the full Knowledge Base from dataset_v2.jsonl, dataset_v3.jsonl, and dataset_v4.jsonl into the Vector DB. Achieved high RAG retrieval accuracy through Metadata and Chunking optimization. Source code (lambda_functions.py, prompts_master.py) is ready for deployment to AWS Lambda next week. Cost Management:\nConfirmed the cost-effectiveness of Serverless Vector DB solutions compared to self-hosted EC2/RDS options for the initial phase. "
},
{
	"uri": "//localhost:1313/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "SorcererXtreme: Building an AI-Powered Interpretation Platform on AWS Overview SorcererXtreme AI is a pioneering metaphysical guidance platform that leverages AI and AWS Serverless architecture to provide personalized, grounded, and reliable readings in Astrology, Tarot, Horoscopes, and Numerology.\n1. Frontend Development This section focuses on building the React/Next.js user interface and integrating with AWS Amplify.\nObjective Technology \u0026amp; Concepts Output Product Interface \u0026amp; UX Build core components (Chat UI, Profile Settings, Payment Gateways). Intuitive, responsive user interface for services: Tarot Reading, Astrology Chart. Authentication Integrate AWS Cognito and Amplify Authenticator into the Frontend. Fully functional sign-up/sign-in system. Backend Integration Write Client-side API calls (axios) functions to invoke the API Gateway Endpoint (Backend). Working Fetch data/Post requests functions, displaying data (Response) from Lambda. 2. Backend Development This is the core of the Serverless architecture, focusing on business logic and performance optimization.\nObjective Technology \u0026amp; Concepts Output Product API \u0026amp; DB Layer Set up Express.js and serverless-http on AWS Lambda. Configure Prisma and secure connection to NeonDB. Basic Lambda functions working: UserAPI (Profile CRUD) and ReminderService. Asynchronous Architecture Build the Reminder Service flow using EventBridge Scheduler and Amazon SES. The findUsersToRemind logic is operational and automatically sends notification emails. Optimization Optimize the Serverless/Prisma deployment package, and set up minimum IAM Roles. Backend source code is automatically deployed via GitHub Actions (CI/CD). 4. AI Development The most crucial part, where the RAG logic is established to deliver intelligent content.\nObjective Technology \u0026amp; Concepts Output Product RAG Core Understand and set up the Retrieval-Augmented Generation (RAG) flow. Operational RAG architecture. Bedrock \u0026amp; Embeddings Use Amazon Bedrock to create Vector Embeddings from user questions and call LLMs. The Lambda function (ChatbotAPI) successfully calls Bedrock to generate answers. Data Retrieval Use Pinecone as a Vector Database to store and retrieve knowledge Chunks from the RAG knowledge base (stored in S3). The process of context retrieval and passing that context into the Prompt to generate accurate answers. Content Workshop overview Frontend Development Backend Development AI Development "
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.6-backend-architecture/",
	"title": "Backend Reference Architecture (RAG &amp; Database)",
	"tags": [],
	"description": "",
	"content": "As a modern Frontend Developer, the line between Frontend and Backend is blurring. You don\u0026rsquo;t need to write SQL or manage servers, but you usually must understand the data flow to integrate effectively.\nThe SorcererXtreme system uses an advanced RAG (Retrieval-Augmented Generation) architecture. Let\u0026rsquo;s dissect what happens after you call fetch('/api/chat').\n1. RAG Flow: Why is the AI accurate? If we simply send the question to ChatGPT, it might \u0026ldquo;hallucinate\u0026rdquo; based on old training data. To make the AI act like a \u0026ldquo;Wise Sorcerer\u0026rdquo; knowledgeable in Tarot, we use a 4-step process:\nRetrieval: When User asks \u0026ldquo;What does The Fool card mean?\u0026rdquo;, the question is converted into a vector (numbers). The system searches Pinecone (Vector DB) for Tarot book passages with the most similar meaning. Augmentation: The system combines the original question + found content from Pinecone into a complete Prompt. Prompt: \u0026ldquo;Based on the following knowledge [book excerpt\u0026hellip;], answer the question: What does The Fool card mean?\u0026rdquo; Generation: Send this Prompt to Amazon Bedrock (hosting Claude 3 or Titan models). AI answers based strictly on the provided text, avoiding fabrication. Response: Frontend receives the final answer and renders it. 2. \u0026ldquo;Polyglot Persistence\u0026rdquo; Strategy A large application never uses just one type of Database. We use the right tool for the right job:\nA. NeonDB (Serverless PostgreSQL)\nType: Relational. Data: User Profiles, VIP tiers, Payment transactions. Why: Financial data requires the highest integrity (ACID). SQL is the #1 choice. B. Amazon DynamoDB\nType: NoSQL (Key-Value). Data: Chat History, Activity Logs. Why: Chat generates millions of records. DynamoDB offers limitless scale with single-digit millisecond latency. C. Pinecone\nType: Vector Database. Data: Tarot/Astrology Knowledge (encoded as Vectors). Why: SQL or NoSQL cannot search by \u0026ldquo;meaning\u0026rdquo; (semantic search). Only a Vector DB understands that \u0026ldquo;King of Coins\u0026rdquo; and \u0026ldquo;King of Pentacles\u0026rdquo; are related. 3. Security: The \u0026ldquo;Fortress\u0026rdquo; Model Your Frontend (sorcererxtreme.vn) is public land. But the Backend is a sanctuary.\nNo Exposed Databases: NeonDB or Pinecone never open ports to the Internet. API Gateway as the Guard: All requests must pass through API Gateway. It checks the \u0026ldquo;ID Badge\u0026rdquo; (Cognito Token). If missing or expired -\u0026gt; Block immediately (401 Unauthorized). Lambda as the Courier: Only Lambda functions (residing in a private VPC) hold the Secret Keys to unlock the Database doors. Takeaway: When debugging \u0026ldquo;why did the AI give a wrong answer?\u0026rdquo;, a Frontend Dev can guess: \u0026ldquo;Ah, maybe the Retrieval step in Pinecone found the wrong context\u0026rdquo;, instead of blaming the AI Model. Understanding the system helps you fix bugs faster!\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.6-microservice/",
	"title": "Connect microservices",
	"tags": [],
	"description": "",
	"content": "In a Serverless architecture, when one Lambda function (e.g., Chatbot) needs to call another service (MetaphysicalAPI), the best approach is to use an HTTP request through that service\u0026rsquo;s API Gateway.\n1. Principles of API Calling Endpoint: Always call through the public API Gateway Endpoint (or an internal one if both Lambdas are within a VPC and use a Private API Gateway). Library: Use a familiar HTTP library like axios to manage the request and client-side timeout. Security: Use IAM Roles or API Keys to authenticate calls between services if the API Gateway is not entirely public. 2. Timeout Management Managing the timeout is the most critical factor to prevent 504 Gateway Timeout errors.\n[Image of AWS API Gateway 29-second timeout limit diagram]\nAPI Gateway Limit: The API Gateway has a hard timeout limit of 29 seconds. Any request lasting longer than 29 seconds is cut off and returns a 504 error. Lambda Limit: Lambda can run up to 15 minutes, but it is constrained by the API Gateway. Therefore, the Lambda timeout must be set less than 29 seconds (e.g., 25 seconds). Configuration Location Recommended Value Rationale API Gateway Timeout serverless.yml (Provider level) 29 seconds AWS maximum hard limit. Lambda Timeout (Receiver) serverless.yml (Function level) 25 seconds Must be less than 29s so Lambda can return an error before the API Gateway cuts the connection. Client Timeout (axios) TypeScript Code 25,000 ms (25 seconds) Ensures the client cuts the connection before Lambda times out, allowing for cleaner client-side error handling. 3. Lambda Performance Optimization If your AI Service performs heavy tasks (like RAG, astronomical calculations), optimizing processing speed is mandatory.\nIncrease Memory (RAM): Increase the memorySize of the AI Service Lambda to a higher level (e.g., 1536 MB or 3008 MB). On AWS, increasing RAM also increases CPU and Network Bandwidth, significantly speeding up heavy processing and reducing the likelihood of 504 errors. Code Configuration: Configure timeout: 25 in serverless.yml for both the Backend and the AI Service. 4. Code Example Here is the AI calling function, modified to handle exceptions clearly and adhere to the Timeout principle.\nimport axios from \u0026#39;axios\u0026#39;; // Assume AI_SERVICE_URL is retrieved from AWS SSM Parameter Store and injected into ENV const AI_SERVICE_URL = process.env.AI_SERVICE_URL; const CLIENT_TIMEOUT_MS = 25000; // 25 seconds (Below the 29s API Gateway limit) /** * Calls the AI service\u0026#39;s API Gateway to receive a response. * @param prompt The request data sent to the AI. */ export const callAI = async (prompt: string, userId: string) =\u0026gt; { if (!AI_SERVICE_URL) { throw new Error(\u0026#34;AI Service URL is not configured.\u0026#34;); } try { const res = await axios.post(AI_SERVICE_URL, { prompt, userId // Send user ID so the AI Service can log or handle VIPs }, { timeout: CLIENT_TIMEOUT_MS // Set client timeout }); // Handle successful response return res.data; } catch (error) { if (axios.isAxiosError(error) \u0026amp;\u0026amp; error.code === \u0026#39;ECONNABORTED\u0026#39;) { // Client-side Timeout error console.error(\u0026#34;AI Timeout Error: Request took too long.\u0026#34;); throw new Error(\u0026#34;AI Service timed out (504). Please try again.\u0026#34;); } // Other errors (e.g., 4xx, 5xx from AI Service) console.error(\u0026#34;AI Service Error:\u0026#34;, error.message); throw new Error(\u0026#34;AI Service unavailable or returned an error.\u0026#34;); } }; "
},
{
	"uri": "//localhost:1313/5-workshop/5.4-ai-development/5.4.6-roadmap/",
	"title": "Lessons &amp; Roadmap",
	"tags": [],
	"description": "",
	"content": "1. Technical Challenges \u0026amp; Critical Lessons Building a metaphysical AI consulting system is not just about assembling AWS services; it is a battle for Data Quality and Semantics.\nThe Issue of Information Noise (RAG Hallucination): Reality: Metaphysical data is often abstract. When a user asks a vague question (e.g., \u0026ldquo;How is my future?\u0026rdquo;), Vector Search easily returns irrelevant results (Noise), causing the LLM to \u0026ldquo;hallucinate\u0026rdquo; and fabricate answers. Lesson: The quality of the Knowledge Base is more important than quantity. Chunking data via .jsonl structure and meticulous Metadata Tagging are the keys to increasing Precision. Language Challenges (Vietnamese Nuance): Reality: International Embedding models sometimes fail to fully grasp Sino-Vietnamese terms in Horoscopes (e.g., \u0026ldquo;Cung Mệnh\u0026rdquo;, \u0026ldquo;Thiên Di\u0026rdquo;). Solution: Use of cohere.embed-multilingual-v3 combined with Hybrid Search (pairing DynamoDB\u0026rsquo;s exact keyword search with Pinecone\u0026rsquo;s semantic search) to compensate for this deficiency. 2. Future Roadmap The next objective is not just for the AI to answer correctly, but to answer \u0026ldquo;correctly specifically for YOU.\u0026rdquo; The system will shift from general consulting to identity-based consulting.\nUser Feedback Loop (RLHF Lite) Implement a Like/Dislike mechanism for each response. This data will be recorded to:\nRefine Prompts (Prompt Tuning). Filter out low-quality RAG data segments from Pinecone. 3. Closing The current system has established a solid foundation: Serverless for cost optimization, Pinecone for memory processing, and Python to connect everything.\nThe shift towards Personalization will be a quantum leap, transforming the system from a \u0026ldquo;Search Engine\u0026rdquo; into a true \u0026ldquo;Spiritual Companion,\u0026rdquo; capable of deep understanding and companionship with the user.\n"
},
{
	"uri": "//localhost:1313/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at AWS First Cloud Journey from September 4, 2025 to December 09, 2025, I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI directly participated in building and deploying the Serverless architecture for the SorcererXtreme project. Through this process, I gained specific experiences:\nTechnical Skills: Mastered the operation and integration of core AWS services (AWS Lambda, API Gateway, DynamoDB), practiced Infrastructure as Code (IaC) for infrastructure management, and built CI/CD pipelines for automated testing and deployment. Mindset: Developed a mindset for designing scalable systems and began addressing Cloud Cost Optimization challenges. Soft Skills: Honed effective Teamwork skills within an Agile/Scrum environment, along with transparent task management and progress reporting. In terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ✅ ☐ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ☐ ✅ ☐ 5 Discipline Adhering to schedules, rules, and work processes ✅ ☐ ☐ 6 Progressive mindset Willingness to receive feedback and improve oneself ✅ ☐ ☐ 7 Communication Presenting ideas and reporting work clearly ✅ ☐ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ✅ ☐ ☐ Needs Improvement To develop into a more professional engineer, I recognize the need to focus on improving the following areas:\n1. Technical Depth Optimization Mindset: Move beyond just \u0026ldquo;making it work\u0026rdquo; to focusing on Performance Optimization and Cost Optimization for the AWS services used in the project. Security Awareness: Apply security principles (IAM roles, Security Groups) more strictly and proactively from the early development stages. 2. Workflow \u0026amp; Process Documentation Discipline: Maintain a habit of writing clear Technical Documentation and comprehensive code comments to facilitate future maintenance. Time Management: Improve the ability to estimate time for complex tasks to ensure more accurate deadlines. 3. Soft Skills Proactive Communication: Report blockers or issues earlier instead of trying to solve them alone for too long. Technical English: Enhance the ability to comprehend in-depth documentation and communicate technical concepts in English. "
},
{
	"uri": "//localhost:1313/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Master Serverless Architecture: Proficiently build, deploy, and trigger AWS Lambda functions via Amazon API Gateway. Migration: Move the entire AI/RAG logic from the local environment to the Cloud. Dependency Management: Solve the challenge of heavy Python libraries (LangChain, Pinecone) using Lambda Layers. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Code Refactoring: Rewrite lambda_functions.py to adhere to the AWS lambda_handler standard, separating Tarot and Horoscope logic. - Migrate hardcoded configurations to Environment Variables. 13/10/2025 13/10/2025 lambda_functions.py 3 - Create Lambda Layers: Package heavy libraries like openai, pinecone-client, and langchain into Layers to reduce deployment package size and improve Cold Start time. 14/10/2025 14/10/2025 AWS Docs 4 - Deploy Lambda Function: Upload code and attach Layers. Integrate prompts_master.py to ensure the AI maintains its \u0026ldquo;Fortune Teller\u0026rdquo; persona. - Test functions directly on the AWS Console. 15/10/2025 15/10/2025 prompts_master.py 5 - Configure API Gateway: Create REST APIs as entry points for the Frontend to invoke Lambda. - Set up authentication (API Key or Cognito Authorizer) to secure the API. 16/10/2025 16/10/2025 https://aws.amazon.com/api-gateway/ 6 - Performance Optimization: Adjust Memory (more RAM = more CPU) and Timeout settings (RAG can take \u0026gt;10s) to prevent premature termination. - 9:00 PM Team Meeting: Demo API Endpoints for Backend and Frontend integration. 17/10/2025 17/10/2025 Internal Team 7 - Debug integration issues (CORS, malformed JSON). - Write Week 6 Worklog. 18/10/2025 18/10/2025 Week 6 Achievements: AI Application (Serverless):\nSuccessfully migrated logic from lambda_function.py and lambda_functions.py to AWS Lambda. Solved dependency management issues using Lambda Layers. The AI system is now accessible via the Internet through API Gateway, moving away from localhost. Optimization:\nConfigured appropriate Timeouts (e.g., 30s-60s) to ensure complex RAG tasks complete successfully without unnecessary cost overruns. Teamwork:\nProvided API documentation (Endpoints, Request/Response body) to team members for system integration. "
},
{
	"uri": "//localhost:1313/5-workshop/5.2-frontend-development/5.2.7-cleanup/",
	"title": "Cleanup Resources",
	"tags": [],
	"description": "",
	"content": "After completing the Workshop, cleaning up is mandatory to avoid unexpected AWS charges. Since we use Serverless Services (Amplify, Lambda, Bedrock), cleanup is straightforward but requires attention.\n1. Delete AWS Amplify App This is the most critical step. Deleting the App on Amplify automatically cleans up 80% of related resources (S3 Hosting, CloudFront, CI/CD Pipeline).\nAccess AWS Amplify Console. Select the SorcererXtreme app. Click Actions tab (top right) -\u0026gt; Select Delete app. Type the confirmation phrase (usually delete) and click Confirm. 2. Cleanup Database \u0026amp; External Services Since NeonDB and Pinecone are 3rd party services (not included in Amplify Delete), you must delete them manually:\nNeonDB: Log in to Neon Console. Go to Project Settings -\u0026gt; Delete Project. Pinecone: Log in to Pinecone Console. Delete the Index (e.g., tarot-knowledge-base) to stop vector storage billing. 3. Manual AWS Resource Cleanup (If created separately) If you created extra resources outside of Amplify during the workshop, check:\nAmazon Bedrock: Bedrock is billed On-demand per request, so you don\u0026rsquo;t need to \u0026ldquo;delete\u0026rdquo; Models. However, if you created a custom Knowledge Base, delete it. Amazon Cognito: Check if the User Pool is gone (Amplify usually deletes it). Parameter Store: Go to AWS Systems Manager \u0026gt; Parameter Store \u0026gt; Delete keys like /sorcerer/neon_db_url, /sorcerer/pinecone_api_key. 4. Final Switch (Billing Dashboard) To be 100% sure:\nAccess AWS Billing Dashboard. Check the \u0026ldquo;Bills\u0026rdquo; section. Wait 24h for the system to update and ensure no new charges are appearing from unknown services. Conclusion Congratulations on reaching the end of the journey!\nYou have successfully built SorcererXtreme - an application blending Frontend artistry (Next.js), Cloud power (AWS Amplify), and Artificial Intelligence (Bedrock RAG).\nSee you in advanced Workshops!\n"
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.7-email/",
	"title": "Setup Email",
	"tags": [],
	"description": "",
	"content": "To ensure the highest delivery reliability for project emails and prevent them from being marked as Spam by email service providers (like Gmail, Outlook), domain security configuration is mandatory.\n1. Basic Configuration and Identity Security Step Action Purpose 1. Domain Purchase Use a private domain (.com, .xyz) instead of a shared one. Reputation: Establish a separate email sending reputation, avoiding impact from other bad senders. 2. Verify Domain in AWS SES Access SES -\u0026gt; Verified Identities -\u0026gt; Create Identity (Select Domain). Proof of Ownership: Allows AWS SES to manage email sending on behalf of your domain. 3. Configure DNS (Email Security) Add CNAME (DKIM), TXT (SPF), and TXT (DMARC) records. Crucial: These records verify the sending source (DKIM/SPF) and instruct the receiving server how to handle invalid emails (DMARC), preventing emails from falling into Spam. Details of Mandatory DNS Records: DKIM (CNAME): Add the 3 CNAME records provided by AWS SES. Purpose: Digital Signature. SPF (TXT): Add a TXT record with the content: v=spf1 include:amazonses.com ~all. Purpose: Designates AWS SES as an authorized server to send mail. DMARC (TXT): Add a TXT record (usually as _dmarc) with the content: v=DMARC1; p=none;. Purpose: Establishes reporting and handling policies for fraudulent emails. 2. Setting up the Sending Flow and Exiting Sandbox After the domain has been verified (Verification Status: Verified), you need to set up the operational flow and request actual sending permission.\nStep Interacting Service Action 1. Activate Async Flow EventBridge Scheduler -\u0026gt; Lambda (TriggerReminder) The asynchronous flow starts according to a defined schedule. 2. Send SES Request Lambda (TriggerReminder) -\u0026gt; Amazon SES API The Lambda function (with the granted IAM Role) directly calls the SES API (e.g., SendEmailCommand) to send personalized emails to each user. 3. Request Production Access AWS Support Center Submit a ticket requesting AWS to upgrade your account from Sandbox mode so you can send emails to any unverified address. "
},
{
	"uri": "//localhost:1313/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "1. Professional Growth \u0026amp; Skills 1. Knowledge \u0026amp; Technology\nTech stack: Over the past 3 months, I have truly mastered AWS Core Services (EC2, S3, RDS) and Infrastructure as Code (Terraform) – topics I previously only knew in theory. Additionally, I got hands-on experience with CI/CD pipelines, an area that AI students like me often overlook. Theory vs. Reality: At university, as long as the code runs, it\u0026rsquo;s done (Happy Case). At FCJ, I learned that running code is not enough; it must be cost-optimized, secure, and scalable. Biggest Challenge: Migrating a database from on-premise to the Cloud. Handling data synchronization without causing service downtime was the toughest problem I solved. 2. Soft Skills \u0026amp; Workflow\nSkills: The skill of \u0026ldquo;Asking the right questions\u0026rdquo; is what I honed the most. Instead of just asking \u0026ldquo;Why is this broken?\u0026rdquo;, I learned to frame it as \u0026ldquo;I tried A and B, but got result C. Is my approach correct?\u0026rdquo;. Workflow: I really appreciate the strict Code Review process. The feedback from seniors didn\u0026rsquo;t just catch syntax errors but also suggested ways to write cleaner, more optimized code. 2. People \u0026amp; Culture 3. Mentorship Experience\nStyle: My mentor follows a \u0026ldquo;Coaching\u0026rdquo; style – never giving the answer immediately but asking leading questions so I can find the solution myself. This was challenging at first but helped me retain knowledge effectively. Impression: The mentor\u0026rsquo;s calmness during system incidents. Instead of panicking, he guided the team step-by-step to trace logs and fix the issue systematically. 4. Culture \u0026amp; Connection\nAtmosphere: The working environment is very flat and open. I felt comfortable debating technical solutions with Mentors without fear of judgment, as long as I had logical arguments. Events: Beyond the \u0026ldquo;day in the life\u0026rdquo; office experiences, FCJ and the seniors organized many professional sharing sessions, helping everyone gain a deeper and broader perspective on AWS. 3. Operations \u0026amp; Support 5. Admin Team \u0026amp; FCJ Team Role\nOnboarding: The internal \u0026ldquo;Cloud Journey\u0026rdquo; documentation is incredibly detailed. It even predicts common errors and provides ready-made solutions for newbies. Support: The Admin team is very enthusiastic. Just be brave enough to ask, and you will definitely get support. 6. Facilities\nWorkspace: Open office layout with comfortable ergonomic chairs. Although the space was occasionally a bit crowded, the atmosphere remained pleasant and never stifling. 4. Retrospective \u0026amp; Suggestions Key Takeaways\nThe most memorable (and painful) lesson: Every mistake on the Cloud costs actual money. Creating AWS services and forgetting to delete resources is a sure way to make your wallet \u0026ldquo;evaporate\u0026rdquo; in the blink of an eye.\nAreas for Improvement\nImprovement: The program should add a workshop on \u0026ldquo;Technical Writing,\u0026rdquo; as I noticed many interns (including myself) are still weak in this area. Training: I hope to have more in-depth sharing sessions on specific service techniques in future batches. Final Thoughts\nThank you, FCJ team, for giving me a memorable experience and wonderful colleagues! "
},
{
	"uri": "//localhost:1313/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: CI/CD Implementation: Establish a Continuous Integration/Continuous Deployment pipeline using GitHub Actions to automate Lambda code updates. DNS \u0026amp; CDN: Configure custom domains with Route 53 and accelerate content delivery using CloudFront. Environment Management: Securely manage environment variables during the deployment process. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Setup GitHub Actions: Write .yml workflow files to automate steps: Checkout code, install Python dependencies, and deploy to AWS Lambda upon push to main. - Configure GitHub Secrets to securely store credentials (AWS Credentials, Pinecone API Key, Neon DB URL). 20/10/2025 20/10/2025 GitHub Actions Docs 3 - Pipeline Testing: Test the CI/CD pipeline with lambda_functions.py. Verify that code changes are automatically reflected in the AWS Console. - Resolve IAM permission issues for the GitHub Actions Runner (ensure permission to update Lambda code). 21/10/2025 21/10/2025 AWS IAM 4 - Configure Route 53: Register/Configure a Hosted Zone for the project API (e.g., api.tamlinh.com). - Point DNS records (A Record/Alias) to the API Gateway. 22/10/2025 22/10/2025 https://aws.amazon.com/route53/ 5 - Configure CloudFront: Create a Distribution to serve static assets like Tarot card images (stored in S3) via CDN to reduce latency for end-users. - Setup SSL/TLS Certificate (ACM) to enable HTTPS. 23/10/2025 23/10/2025 https://aws.amazon.com/cloudfront/ 6 - 9:00 PM Team Meeting: Demo the \u0026ldquo;One-click Deploy\u0026rdquo; process. Guide team members on how to check build logs on GitHub. - Agree on branch naming conventions and Merge Request (MR) processes. 24/10/2025 24/10/2025 Internal Team 7 - Security review of the Pipeline (ensure no Keys are exposed in logs). - Write Week 7 Worklog. 25/10/2025 25/10/2025 Week 7 Achievements: DevOps Process:\nSuccessfully built a CI/CD pipeline with GitHub Actions. Updating AI logic (like tweaking prompts in prompts_master.py) now only requires a code push for automatic deployment. Eliminated risks associated with manual deployment errors. Secured sensitive information (API Keys) using GitHub Secrets instead of hardcoding. Network Infrastructure:\nThe API is now accessible via a professional, easy-to-remember domain secured with SSL. Significant improvement in loading speed for Tarot images and static assets due to CloudFront caching. "
},
{
	"uri": "//localhost:1313/5-workshop/5.3-backend-development/5.3.8-summary/",
	"title": "Development workflow",
	"tags": [],
	"description": "",
	"content": "The workflow is designed to leverage the speed of local development and the safety and automation of the Serverless architecture.\nStep Activity Environment Role and Notes 1. Code Development Code logic, API modifications (TypeScript/Express). Local Machine Use VS Code and Node.js libraries. 2. Local Testing Test synchronous APIs. sls offline start Simulates the Lambda/API Gateway environment. Connects directly to NeonDB via the local .env file. 3. Database Schema Update If Schema is modified (schema.prisma). npx prisma migrate dev Creates Migration and applies changes. 4. Commit \u0026amp; Push Code Commit code and push to the repository. git push origin main Triggers the automated CI/CD flow. 5. Automated CI/CD Deployment to the Cloud. GitHub Actions Automated: Build -\u0026gt; Prisma Generate -\u0026gt; AWS/SSM Login (fetch Keys) -\u0026gt; Deploy to AWS Lambda. 6. Error Monitoring Check Real-time Logs. sls logs -f api -t Use the Serverless Framework command to immediately view CloudWatch Logs and debug errors in the Production/Staging environment. Important Notes on Prisma: Use migrate dev: Instead of db push (which is only for non-production/test), you should use npx prisma migrate dev to create a history of changes (migration files) and apply them to NeonDB. Generate on CI/CD: Running npx prisma generate in GitHub Actions is mandatory to download the necessary binaries (rhel-openssl-3.0.x) for the AWS Lambda Linux environment. "
},
{
	"uri": "//localhost:1313/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Decoupling: Implement Amazon SQS and SNS to decouple application components, ensuring stability under high load. Asynchronous Processing: Shift AI inference logic (RAG) to a Queue-based model to prevent API Gateway timeouts. Data Pivot: Execute a strategic change in the Astrology data source to improve reading accuracy. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Research SQS/SNS: Understand Pub/Sub patterns and Queue types (Standard vs. FIFO). - Practice: Create an SQS Queue and trigger a Lambda function via Event Source Mapping. 27/10/2025 27/10/2025 https://aws.amazon.com/sqs/ 3 - 9:00 PM Team Meeting (Pivot Discussion): Discuss changing the Astrology data source. Decided to switch from the old dataset_v1.jsonl to the more detailed dataset_v2.jsonl structure for deeper AI insights. - Assess the impact of this data swap on the current RAG pipeline. 28/10/2025 28/10/2025 Internal Team 4 - Architecture Update: Redraw the system diagram on draw.io, adding SQS between API Gateway and the AI processing Lambda. - Refactor Code: Split lambda_handler in lambda_functions.py into a producer (pushes to SQS) and a worker (processes AI). 29/10/2025 29/10/2025 High_Level_System_Architecture.drawio.jpg 5 - New Data Processing: Write scripts to clean and re-vectorize the new Astrology dataset (dataset_v2.jsonl) into Pinecone/Neon. - Update prompts_master.py to accommodate new data fields (e.g., Rising Sign, Descendant). 30/10/2025 30/10/2025 prompts_master.py 6 - Integration Debugging: Fix connection issues between Chatbot and Vector DB caused by the async shift. - 9:00 PM Team Meeting: Demo the new flow: User Request -\u0026gt; Receive ID -\u0026gt; Background Processing -\u0026gt; Notification. 31/10/2025 31/10/2025 Internal Team 7 - Review cost implications of SQS and increased Lambda invocations (workers). - Write Week 8 Worklog. 01/11/2025 01/11/2025 Week 8 Achievements: Architecture:\nSuccessfully implemented Amazon SQS as a buffer for heavy AI inference requests, preventing system overload. Improved User Experience (UX): The app now responds immediately (returning a \u0026ldquo;Processing\u0026rdquo; status) instead of making users wait 30-60s for the AI. Data Pivot:\nSuccessfully migrated to a higher-quality Astrology dataset (dataset_v2.jsonl), resulting in more detailed and authentic AI readings. The RAG pipeline is stable with the new data structure. Source Code:\nOptimized lambda_functions.py logic for asynchronous processing. "
},
{
	"uri": "//localhost:1313/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Scheduled Tasks: Implement the \u0026ldquo;Daily Horoscope\u0026rdquo; feature using Amazon EventBridge for scheduling and Amazon SES for email delivery. Security \u0026amp; Identity: Integrate Amazon Cognito for user management and AWS Secrets Manager to protect sensitive API Keys. Architecture Finalization: Finalize and approve the High-Level Architecture (HLA) diagram. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Configure Cognito: Create a User Pool for app authentication and integrate it with API Gateway Authorizer. - Security: Migrate API Keys (OpenAI, Pinecone) from environment variables to AWS Secrets Manager. 03/11/2025 03/11/2025 AWS Cognito \u0026amp; Secrets Manager 3 - Setup SES: Configure Amazon SES (Simple Email Service), verify sender domain/email. - Implement the send_email function in lambda_functions.py integrated with SES. 04/11/2025 04/11/2025 https://aws.amazon.com/ses/ 4 - Create Cron Job: Configure EventBridge Scheduler to trigger the DailyEmailJob Lambda at 6:00 AM daily. - Update prompts_master.py with a new prompt template for \u0026ldquo;Daily Forecast\u0026rdquo;. 05/11/2025 05/11/2025 AWS EventBridge 5 - Integration Testing: Verify the flow: EventBridge -\u0026gt; Lambda -\u0026gt; RAG (get forecast) -\u0026gt; SES (send email). Ensure emails are delivered to Inbox. 06/11/2025 06/11/2025 Internal Team 6 - 9:00 PM Team Meeting: Present the complete Architecture Diagram (updated with SQS, EventBridge, SES, Cognito) for final team approval. - Discuss the Demo scenario for the final week. 07/11/2025 07/11/2025 Internal Team 7 - Audit IAM permissions (Least Privilege) for the newly added services. - Write Week 9 Worklog. 08/11/2025 08/11/2025 Week 9 Achievements: Features:\n\u0026ldquo;Daily Horoscope\u0026rdquo; is now fully automated: The system wakes up at 6 AM, calculates the forecast, and sends emails to users without manual intervention. User Authentication (AuthN/AuthZ) is secured with Cognito. Security:\nEliminated storage of API Keys in code or insecure environment variables by using Secrets Manager. Documentation:\nThe Architecture Diagram has been updated to reflect the full implementation (EventBridge, SES, Cognito) and matches the actual deployment. "
},
{
	"uri": "//localhost:1313/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Production Deployment: Deploy the official Production Environment infrastructure, including VPC, Security Groups, and Database instances. Full Data Vectorization: Process and ingest all datasets into the Vector Database (Pinecone/Neon). Backend Logic Finalization: Finalize the core processing logic for Tarot and Horoscope features in the Production environment. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Infrastructure Setup: Deploy Production VPC, configure strict Security Groups (allow only API Gateway -\u0026gt; Lambda -\u0026gt; DB) based on the approved diagram. - Set up Environment Variables (Prod) for Lambda. 10/11/2025 10/11/2025 AWS VPC 3 - Big Data Processing: Write batch scripts to vectorize all data from dataset_v2.jsonl (Horoscope), dataset_v3.jsonl (Numerology), and dataset_v4.jsonl (Tarot). - Upsert vectors into Pinecone index (Prod). 11/11/2025 11/11/2025 Pinecone Docs 4 - Code Merge: Coordinate with the team to merge Feature branches into Main. Resolve code conflicts in lambda_functions.py. - Run CI/CD pipeline to deploy the latest build. 12/11/2025 12/11/2025 GitHub Actions 5 - RAG Optimization (Prod): Check retrieval speed with the full dataset. Tune the top_k parameter in query_rag to balance accuracy and latency. - Final update to prompts_master.py to ensure consistent tone. 13/11/2025 13/11/2025 local_worker.py 6 - 9:00 PM Team Meeting: Demo the \u0026ldquo;Lifetime Horoscope\u0026rdquo; feature with full data. - Review Business Logic errors before entering the integration testing week. 14/11/2025 14/11/2025 Internal Team 7 - Backup Databases (RDS Snapshot/DynamoDB Export). - Write Week 10 Worklog. 15/11/2025 15/11/2025 Week 10 Achievements: Infrastructure:\nProduction environment is live and fully isolated from Dev/Test. Network and security settings are configured according to the High-Level Architecture (HLA). Data (Core AI):\n100% of Data from datasets (v2, v3, v4) has been cleaned and indexed into the Vector DB. The AI can now provide detailed readings for all Tarot cards and Zodiac signs. RAG logic performs smoothly with the full dataset loaded. Source Code:\nThe codebase is stable; feature branches have been merged and are ready for comprehensive Integration Testing next week. "
},
{
	"uri": "//localhost:1313/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Integration Testing: Ensure seamless connectivity between Frontend and Backend (API Gateway/Lambda) and verify AI system accuracy under load. AI Polish (Persona Tuning): Refine the AI\u0026rsquo;s tone and style to be more mystical and empathetic, aligning with the spiritual theme. Cost Optimization: Audit and adjust AWS/Pinecone resources to minimize operational costs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Load Testing: Use tools (like Locust/JMeter) to simulate high traffic to the Fortune Telling API. Monitor CloudWatch to identify bottlenecks. - Resolve Timeout issues in API Gateway caused by long AI inference times. 17/11/2025 17/11/2025 AWS CloudWatch 3 - Prompt Tuning: Based on test feedback, update prompts_master.py to ensure the AI provides deeper, more spiritual insights rather than short answers. - Add Few-shot prompting examples to prompts.py to better guide the model\u0026rsquo;s responses. 18/11/2025 18/11/2025 prompts.py 4 - Code Optimization: Review lambda_functions.py and remove excessive logging statements to save on CloudWatch Logs storage costs. - Configure Caching on API Gateway for identical requests (e.g., Horoscope for the same sign on the same day). 19/11/2025 19/11/2025 AWS API Gateway 5 - Cost Optimization: Downscale Pinecone Pods or switch to Serverless mode to save costs during idle times. - Tune Lambda Memory settings to the minimum required to execute the local_worker.py logic efficiently. 20/11/2025 20/11/2025 Pinecone Console 6 - 9:00 PM Team Meeting: Code Freeze. No new features will be added; focus shifts entirely to bug fixing. - Assign tasks for recording the demo video and creating the presentation slides. 21/11/2025 21/11/2025 Internal Team 7 - Final check of the CI/CD pipeline on GitHub Actions to ensure the final deployment is error-free. - Write Week 11 Worklog. 22/11/2025 22/11/2025 Week 11 Achievements: Performance \u0026amp; Stability:\nThe system successfully handled 500+ simulated concurrent requests. Latency issues were resolved through caching strategies and Python code optimization. AI Quality:\nThe \u0026ldquo;Virtual Fortune Teller\u0026rdquo; now possesses a natural, mystical, and consistent persona thanks to refined prompts in prompts_master.py. Edge cases (e.g., off-topic user questions) are handled gracefully. Cost Efficiency:\nAchieved a ~30% reduction in projected monthly costs by optimizing Lambda resources and Vector DB configurations. "
},
{
	"uri": "//localhost:1313/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Documentation: Complete technical documentation, deployment guides, and Data Flow diagrams for the AI module. Rehearsal \u0026amp; Demo: Prepare the presentation slide deck, record product demo videos, and conduct dry runs. Handover \u0026amp; Retrospective: Package source code, clean up unused resources, and conduct a project retrospective. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Technical Writing: Update README.md on GitHub with detailed deployment instructions (including Secrets setup, CI/CD pipeline execution). - Redraw detailed Data Flow Diagrams for the RAG component to include in the final report. 24/11/2025 24/11/2025 Internal Team 3 - Slide Preparation: Draft the \u0026ldquo;AI Architecture \u0026amp; Challenges\u0026rdquo; section of the slide deck, highlighting the SQS asynchronous solution and Pinecone cost optimization. - Collaborate with Frontend to screen-record key features (Tarot, Horoscope, Chatbot). 25/11/2025 25/11/2025 Template Slides 4 - Video Production: Edit and add subtitles/voiceover to the demo video. Ensure the AI\u0026rsquo;s natural response capability is showcased. - Team Dry Run: Rehearse the full presentation, manage timing, and practice Q\u0026amp;A defense. 26/11/2025 26/11/2025 Video Editor 5 - Final Polish: Check UI/UX one last time to ensure no typos or display errors with long AI responses. - Verify all Production API endpoints to ensure the system is Live and healthy. 27/11/2025 27/11/2025 AWS Console 6 - Final Presentation: Present and Live Demo the product to Mentors/Judges. - Team Retrospective: Meeting to discuss lessons learned, what went well, and areas for improvement over the 12 weeks. 28/11/2025 28/11/2025 Zoom/Meeting 7 - Resource Cleanup: Delete unused AWS resources (test EC2s, junk S3 buckets, old Snapshots) to prevent post-project billing. - Finalize Week 12 Worklog and submit the summary report. 29/11/2025 29/11/2025 AWS Billing Week 12 Achievements: Product:\nThe AI-powered Spiritual App is 100% complete and running stably on the Production environment (AWS Serverless). High-quality demo video produced, clearly showcasing key features: Accurate RAG, Persona-driven Chatbot, and responsive system. Documentation \u0026amp; Knowledge:\nComprehensive technical documentation on GitHub, allowing anyone to understand and further develop the project. Finalized High-Level Architecture (HLA) and Data Flow diagrams. Journey Summary:\nSuccessfully fulfilled the AI Engineer role: From starting with raw data confusion to building a complete Serverless RAG system, fully automated via CI/CD and cost-optimized. "
},
{
	"uri": "//localhost:1313/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "//localhost:1313/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]